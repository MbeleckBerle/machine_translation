{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## The dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "the dataset is WMT-14 English-German translation data from https://nlp.stanford.edu/projects/nmt/. There over 4.5 million sentence pairs available. However, I will only use 10k pairs due to computational feasiblility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  Class EA-JA ; Our wide range of automatic cars include : Renault Clios , Opel Vectras and Ford ...\n",
      "German:  Klasse EA-JA ; unser breites Sortiment an Autos mit Automatik beinhalten : Renault Clio , Opel ... \n",
      "\n",
      "English:  Are you looking for a partner who will guide you , reliably and competently , every step of the way , from your first decision to buy , through the planning stage , to a complete aftersales service ?\n",
      "German:  Sie wünschen sich einen Partner , der Sie von der ersten Entscheidung über die Planung bis hin zum kompletten Service verlässlich und kompetent begleitet ? \n",
      "\n",
      "English:  Aquasphere manufactures and sells a comprehensive line of pool accessories , spare parts for pools , water treatment systems and pool care products .\n",
      "German:  Aquasphere stellt eine umfangreiche Reihe an Schwimmbeckenzubehör , Ersatzteilen und Wasserpflegesystemen her und vermarktet diese . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "n_sentences = 10000\n",
    "\n",
    "# Loading English train sentences\n",
    "original_en_sentences = []\n",
    "with open(os.path.join(\"data\", \"train_10k.en\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for i, row in enumerate(en_file):\n",
    "        if i >= n_sentences:\n",
    "            break\n",
    "        original_en_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "# loading German train sentences\n",
    "original_de_sentences = []\n",
    "with open(os.path.join(\"data\", \"train_10k.de\"), \"r\", encoding=\"utf-8\") as de_file:\n",
    "    for i, row in enumerate(de_file):\n",
    "        if i >= n_sentences:\n",
    "            break\n",
    "        original_de_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "# Loading English test sentences\n",
    "oritinal_en_test_sentences = []\n",
    "\n",
    "with open(os.path.join(\"data\", \"test_100.en\"), \"r\", encoding=\"utf-8\") as de_file:\n",
    "    for i, row in enumerate(de_file):\n",
    "        if i >= n_sentences:\n",
    "            break\n",
    "        oritinal_en_test_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "# Loading German test sentences\n",
    "oritinal_de_test_sentences = []\n",
    "with open(os.path.join(\"data\", \"test_100.de\"), \"r\", encoding=\"utf-8\") as de_file:\n",
    "    for i, row in enumerate(de_file):\n",
    "        if i >= n_sentences:\n",
    "            break\n",
    "        oritinal_de_test_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "### displaying random sentences and their respective translations\n",
    "for i in range(3):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(original_en_sentences[index]))\n",
    "    print(\"German: \", \" \".join(original_de_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding special tokens\n",
    "\n",
    "#### I will add \"< s >\" to mark the start of a sentence and \"< /s >\" to mark the end of a sentence\n",
    "\n",
    "This way\n",
    "we prediction can be done for an arbitrary number of time steps. Using < s > as the starting token gives a\n",
    "way to signal to the decoder that it should start predicting tokens from the target language.\n",
    "\n",
    "if < /s > token is not used to mark the end of a sentence, the decoder cannot be signaled to\n",
    "end a sentence. This can lead the model to enter an infinite loop of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  <s> The NH Giustiniano is set in an impressive building housing a range of elegant and spacious rooms and luxurious suites , as well as 8 meeting rooms with a capacity of 180 people . </s>\n",
      "German:  <s> Ein beeindruckendes Gebäude beherbergt das NH Giustiniano mit seinen eleganten , geräumigen Zimmern sowie den luxuriösen Suiten . Darüber hinaus stehen Ihnen 8 Tagungsräume für bis zu 180 Personen zur Verfügung . </s> \n",
      "\n",
      "English:  <s> This section holds the most general questions about PHP : what it is and what it does . </s>\n",
      "German:  <s> Dieses Kapitel beinhaltet allgemeine Fragen zu PHP : Was es ist und was es tut . </s> \n",
      "\n",
      "English Test:  <s> Orlando Bloom and Miranda Kerr still love each other </s>\n",
      "German Test:  <s> Orlando Bloom und Miranda Kerr lieben sich noch immer </s>\n"
     ]
    }
   ],
   "source": [
    "en_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_en_sentences]\n",
    "de_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_de_sentences]\n",
    "test_en_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in oritinal_en_test_sentences]\n",
    "test_de_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in oritinal_de_test_sentences]\n",
    "\n",
    "for i in range(2):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(en_sentences[index]))\n",
    "    print(\"German: \", \" \".join(de_sentences[index]), \"\\n\")\n",
    "\n",
    "print(\"English Test: \", \" \".join(test_en_sentences[0]))\n",
    "print(\"German Test: \", \" \".join(test_de_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting training and validation dataset\n",
    "\n",
    "#### 90% training and 10% validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Explore', 'new', 'NI', 'technologies', 'ranging', 'from', 'a', 'portable', 'handheld', 'sound', 'and', 'vibration', 'analyzer', 'to', 'high-channel', 'microphone', 'arrays', 'and', 'embedded', 'systems', '.', '</s>']\n",
      "['<s>', 'Die', 'zunehmende', 'Flut', 'an', 'Mess-', 'und', 'Simulationsdaten', 'stellt', 'viele', 'Abteilungen', 'und', 'Unternehmen', 'vor', 'eine', 'Herausforderung', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train_en_sentences, valid_en_sentences, train_de_sentences, valid_de_sentences = (\n",
    "    train_test_split(en_sentences, de_sentences, test_size=0.1)\n",
    ")\n",
    "\n",
    "print(train_en_sentences[1])\n",
    "print(train_de_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining sequence leghts fot the two languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9000.000000\n",
       "mean       27.380889\n",
       "std        14.290006\n",
       "min         8.000000\n",
       "5%         11.000000\n",
       "50%        24.000000\n",
       "95%        56.000000\n",
       "max       102.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Getting some basic statistics from the data\n",
    "\n",
    "# convert train_en_sentences to a pandas series\n",
    "pd.Series(train_en_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistic above shows that 5% of english sentences have 11 words, 50% have 24 words, 95% have 56 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9000.000000\n",
       "mean       24.822222\n",
       "std        12.911805\n",
       "min         8.000000\n",
       "5%         11.000000\n",
       "50%        22.000000\n",
       "95%        50.000000\n",
       "max       102.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_de_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistic above shows that 5% of German sentences have 11 words, 50% have 22 words, 95% have 50 words\n",
    "\n",
    "the minimum and maximum number of sentences is 8 and 102 respectively in both languages. However, this will not always be the case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding the sentences with pad_sequences from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "n_en_seq_length = 50\n",
    "n_de_seq_length = 50\n",
    "unk_token = \"<unk>\"\n",
    "\n",
    "train_en_sentences_padded = pad_sequences(\n",
    "    train_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    ")\n",
    "\n",
    "valid_en_sentences = pad_sequences(\n",
    "    valid_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    ")\n",
    "\n",
    "test_en_sentences = pad_sequences(\n",
    "    test_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    ")\n",
    "\n",
    "\n",
    "train_de_sentences_padded = pad_sequences(\n",
    "    train_de_sentences,\n",
    "    maxlen=n_de_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    ")\n",
    "\n",
    "valid_de_sentences = pad_sequences(\n",
    "    valid_de_sentences,\n",
    "    maxlen=n_de_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    ")\n",
    "\n",
    "test_de_sentences = pad_sequences(\n",
    "    test_de_sentences,\n",
    "    maxlen=n_de_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
