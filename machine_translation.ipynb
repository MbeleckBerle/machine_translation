{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## The dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "the dataset is WMT-14 English-German translation data from https://nlp.stanford.edu/projects/nmt/. There are over 4.5 million sentence pairs available. However, I will only use 10k pairs due to computational feasiblility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  If you use Registry Trash Keys Finder successfully .\n",
      "German:  Sie nutzen Registry Trash Keys Finder erfolgreich . \n",
      "\n",
      "English:  The NH Giustiniano is a brand-new hotel in the heart of exclusive Prati , residential and commercial neighbourhood within walking distance of St. Peter ’ s Cathedral , Castel S.Angelo and the Vatican Museums .\n",
      "German:  Sie wohnen im Herzen des exklusiven Stadtteils Prati - von dieser Geschäfts- und Wohngegend gelangen Sie zu Fuß zum Petersdom , der Engelsburg ( Castel Sant &apos; Angelo ) und den Vatikanischen Museen . \n",
      "\n",
      "English:  Guests with cars will find the A.C. Hotel Hoferer conveniently close to the A8 motorway , and can also park for free on site .\n",
      "German:  Das A.C. Hotel Hoferer liegt günstig nahe der Autobahn A8 und verfügt über kostenfreie Parkplätze . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "n_sentences = 10000\n",
    "\n",
    "# Loading English train sentences\n",
    "original_en_sentences = []\n",
    "with open(\n",
    "    os.path.join(\"./data/en-de\", \"train_10k.en\"), \"r\", encoding=\"utf-8\"\n",
    ") as en_file:\n",
    "    for i, row in enumerate(en_file):\n",
    "        # if i >= n_sentences:\n",
    "        #     break\n",
    "        original_en_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "# loading German train sentences\n",
    "original_de_sentences = []\n",
    "with open(\n",
    "    os.path.join(\"./data/en-de\", \"train_10k.de\"), \"r\", encoding=\"utf-8\"\n",
    ") as de_file:\n",
    "    for i, row in enumerate(de_file):\n",
    "        # if i >= n_sentences:\n",
    "        #     break\n",
    "        original_de_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "# Loading English test sentences\n",
    "oritinal_en_test_sentences = []\n",
    "\n",
    "with open(\n",
    "    os.path.join(\"./data/en-de\", \"test_100.en\"), \"r\", encoding=\"utf-8\"\n",
    ") as de_file:\n",
    "    for i, row in enumerate(de_file):\n",
    "        # if i >= n_sentences:\n",
    "        #     break\n",
    "        oritinal_en_test_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "# Loading German test sentences\n",
    "oritinal_de_test_sentences = []\n",
    "with open(\n",
    "    os.path.join(\"./data/en-de\", \"test_100.de\"), \"r\", encoding=\"utf-8\"\n",
    ") as de_file:\n",
    "    for i, row in enumerate(de_file):\n",
    "        # if i >= n_sentences:\n",
    "        #     break\n",
    "        oritinal_de_test_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "### displaying random sentences and their respective translations\n",
    "for i in range(3):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(original_en_sentences[index]))\n",
    "    print(\"German: \", \" \".join(original_de_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding special tokens\n",
    "\n",
    "#### I will add \"< s >\" to mark the start of a sentence and \"< /s >\" to mark the end of a sentence\n",
    "\n",
    "This way\n",
    "we prediction can be done for an arbitrary number of time steps. Using < s > as the starting token gives a\n",
    "way to signal to the decoder that it should start predicting tokens from the target language.\n",
    "\n",
    "if < /s > token is not used to mark the end of a sentence, the decoder cannot be signaled to\n",
    "end a sentence. This can lead the model to enter an infinite loop of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  <s> The best way to do this is with a link to this web page . </s>\n",
      "German:  <s> Am einfachsten ist es , an entsprechender Stelle einen Link auf diese Seite einzubinden . </s> \n",
      "\n",
      "English:  <s> Fixed an issue where the IME input tool used to enter Japanese , Korean , Chinese and Indic characters was covered by the &quot; Add Bookmark &quot; panel . </s>\n",
      "German:  <s> Die Überdeckung des IME-Eingabeprogramms zur Eingabe von japanischen , koreanischen , chinesischen und indischen Zeichen durch den &quot; Lesezeichen für diese Seite gesetzt &quot; -Dialog wurde behoben . </s> \n",
      "\n",
      "English Test:  <s> Orlando Bloom and Miranda Kerr still love each other </s>\n",
      "German Test:  <s> Orlando Bloom und Miranda Kerr lieben sich noch immer </s>\n"
     ]
    }
   ],
   "source": [
    "en_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_en_sentences]\n",
    "de_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_de_sentences]\n",
    "test_en_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in oritinal_en_test_sentences]\n",
    "test_de_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in oritinal_de_test_sentences]\n",
    "\n",
    "for i in range(2):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(en_sentences[index]))\n",
    "    print(\"German: \", \" \".join(de_sentences[index]), \"\\n\")\n",
    "\n",
    "print(\"English Test: \", \" \".join(test_en_sentences[0]))\n",
    "print(\"German Test: \", \" \".join(test_de_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting training and validation dataset\n",
    "\n",
    "#### 90% training and 10% validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Free', 'public', 'parking', 'is', 'possible', 'at', 'a', 'location', 'nearby', '(', 'reservation', 'is', 'not', 'possible', ')', '.', '</s>']\n",
      "['<s>', 'Öffentliche', 'Parkplätze', 'stehen', 'kostenfrei', 'in', 'der', 'Nähe', '(', 'Reservierung', 'ist', 'nicht', 'möglich', ')', 'zur', 'Verfügung', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train_en_sentences, valid_en_sentences, train_de_sentences, valid_de_sentences = (\n",
    "    train_test_split(en_sentences, de_sentences, test_size=0.1)\n",
    ")\n",
    "\n",
    "print(train_en_sentences[1])\n",
    "print(train_de_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining sequence leghts fot the two languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9000.000000\n",
       "mean       27.419000\n",
       "std        14.356768\n",
       "min         8.000000\n",
       "5%         11.000000\n",
       "50%        24.000000\n",
       "95%        56.000000\n",
       "max       102.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Getting some basic statistics from the data\n",
    "\n",
    "# convert train_en_sentences to a pandas series\n",
    "pd.Series(train_en_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistic above shows that 5% of english sentences have 11 words, 50% have 24 words, 95% have 56 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9000.000000\n",
       "mean       24.821222\n",
       "std        12.896984\n",
       "min         8.000000\n",
       "5%         11.000000\n",
       "50%        22.000000\n",
       "95%        50.000000\n",
       "max       102.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_de_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistic above shows that 5% of German sentences have 11 words, 50% have 22 words, 95% have 50 words\n",
    "\n",
    "the minimum and maximum number of sentences is 8 and 102 respectively in both languages. However, this will not always be the case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding the sentences with pad_sequences from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['<s>', 'If', 'you', 'need', 'that', 'functionality', ',', 'you',\n",
       "       'can', 'eliminate', 'those', 'kinds', 'of', 'layers', 'from',\n",
       "       'your', 'PSD', 'by', 'converting', 'the', 'layer', 'effects', 'to',\n",
       "       'stand-alone', 'layers', 'or', '&apos;', 'smart', 'objects',\n",
       "       '&apos;', '&#91;', 'right', 'click', 'on', 'the', 'Layer', 'in',\n",
       "       'the', 'Photoshop', 'layers', 'palette', '&#93;', '.', '</s>',\n",
       "       '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "n_en_seq_length = 50\n",
    "n_de_seq_length = 50\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "train_en_sentences_padded = pad_sequences(\n",
    "    train_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_en_sentences_padded = pad_sequences(\n",
    "    valid_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_en_sentences_padded = pad_sequences(\n",
    "    test_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "\n",
    "train_de_sentences_padded = pad_sequences(\n",
    "    train_de_sentences,\n",
    "    maxlen=n_de_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_de_sentences_padded = pad_sequences(\n",
    "    valid_de_sentences,\n",
    "    maxlen=n_de_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_de_sentences_padded = pad_sequences(\n",
    "    test_de_sentences,\n",
    "    maxlen=n_de_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_en_sentences_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'und',\n",
       " 'die',\n",
       " 'der',\n",
       " 'in',\n",
       " 'sie',\n",
       " 'von',\n",
       " 'das',\n",
       " 'zu',\n",
       " 'mit',\n",
       " 'ist',\n",
       " 'für',\n",
       " 'den',\n",
       " 'im',\n",
       " 'auf',\n",
       " 'ein',\n",
       " 'des',\n",
       " 'eine',\n",
       " 'dem',\n",
       " 'sich',\n",
       " 'hotel',\n",
       " 'es',\n",
       " 'quot',\n",
       " 'werden',\n",
       " 'an',\n",
       " 'oder',\n",
       " 'nicht',\n",
       " 'als',\n",
       " 'sind',\n",
       " 'auch',\n",
       " 'a',\n",
       " 'ich',\n",
       " 'wird',\n",
       " 'einem',\n",
       " 'über',\n",
       " 'aus',\n",
       " 'einen',\n",
       " 'um',\n",
       " 'the',\n",
       " 'bei',\n",
       " 'zur',\n",
       " 'wie',\n",
       " 'können',\n",
       " 'einer',\n",
       " 'er',\n",
       " 'am',\n",
       " 'nur',\n",
       " 'nach',\n",
       " 'alle',\n",
       " 'so',\n",
       " 'diese',\n",
       " 'kann',\n",
       " 'zimmer',\n",
       " 'zum',\n",
       " 'wir',\n",
       " 'wenn',\n",
       " 'man',\n",
       " 'dieses',\n",
       " 'bis',\n",
       " 'vom',\n",
       " 'durch',\n",
       " 'hat',\n",
       " 'and',\n",
       " 'sehr',\n",
       " 'ihre',\n",
       " 'haben',\n",
       " 'wurde',\n",
       " 'daß',\n",
       " 'war',\n",
       " 'aber',\n",
       " 'bietet',\n",
       " 'pro',\n",
       " 'dass',\n",
       " 'unter',\n",
       " 'vor',\n",
       " 'b',\n",
       " 'was',\n",
       " 'sein',\n",
       " '�',\n",
       " 'liegt',\n",
       " 'ihr',\n",
       " 'ihnen',\n",
       " 'of',\n",
       " 'sowie',\n",
       " 'noch',\n",
       " 'dieser',\n",
       " 'gibt',\n",
       " 'hier',\n",
       " 'entfernt',\n",
       " '1',\n",
       " 'mehr',\n",
       " 'ihrer',\n",
       " 'stadt',\n",
       " '2',\n",
       " 'de',\n",
       " 'denn',\n",
       " 'apos',\n",
       " 'seine',\n",
       " 'windows',\n",
       " 'befindet',\n",
       " 'eines',\n",
       " 'finden',\n",
       " 'to',\n",
       " 'lage',\n",
       " '“',\n",
       " 'euch',\n",
       " 'du',\n",
       " 'damit',\n",
       " 'zwischen',\n",
       " 'zeit',\n",
       " 'hotels',\n",
       " 'c',\n",
       " 'dann',\n",
       " 'diesem',\n",
       " '–',\n",
       " '3',\n",
       " 'uns',\n",
       " 'dies',\n",
       " 'sehen',\n",
       " 'mir',\n",
       " 'da',\n",
       " 'immer',\n",
       " 'keine',\n",
       " 'ihren',\n",
       " '124',\n",
       " 'seit',\n",
       " 'restaurant',\n",
       " '4',\n",
       " 'tag',\n",
       " 'person',\n",
       " 'verfügung',\n",
       " '5',\n",
       " 'software',\n",
       " 'seiner',\n",
       " 'direkt',\n",
       " 'anderen',\n",
       " 'uhr',\n",
       " 'is',\n",
       " 'habe',\n",
       " 'seinen',\n",
       " 'jahren',\n",
       " 'viele',\n",
       " 'gut',\n",
       " 'kinder',\n",
       " 'zwei',\n",
       " 'informationen',\n",
       " 'ihm',\n",
       " 'mein',\n",
       " 's',\n",
       " 'nähe',\n",
       " 'unsere',\n",
       " 'ohne',\n",
       " 'ni',\n",
       " 'welche',\n",
       " 'verfügt',\n",
       " 'herzen',\n",
       " 'gästebewertungen',\n",
       " 'amp',\n",
       " 'lassen',\n",
       " 'einige',\n",
       " 'allen',\n",
       " 'ab',\n",
       " 'einfach',\n",
       " 'andere',\n",
       " 'haus',\n",
       " 'während',\n",
       " 'wurden',\n",
       " 'soll',\n",
       " '„',\n",
       " 'hatte',\n",
       " 'stehen',\n",
       " 'möchten',\n",
       " 'for',\n",
       " 'mich',\n",
       " 'bitte',\n",
       " 'service',\n",
       " 'wo',\n",
       " 'with',\n",
       " 'welt',\n",
       " 'nahe',\n",
       " 'darum',\n",
       " 'zentrum',\n",
       " '10',\n",
       " 'zimmerbeschreibung',\n",
       " 'minuten',\n",
       " 'eur',\n",
       " 'alles',\n",
       " '2009',\n",
       " 'neue',\n",
       " '2000',\n",
       " 'm',\n",
       " 'herr',\n",
       " 'selbst',\n",
       " 'room',\n",
       " 'waren',\n",
       " 'menschen',\n",
       " 'unser',\n",
       " 'strand',\n",
       " 'zahlen',\n",
       " 'bieten',\n",
       " 'nun',\n",
       " 'denen',\n",
       " 'beim',\n",
       " 'werde',\n",
       " 'wann',\n",
       " 'la',\n",
       " 'kommen',\n",
       " 'gegen',\n",
       " 'buchung',\n",
       " 'all',\n",
       " 'sondern',\n",
       " 'verstehen',\n",
       " 'steht',\n",
       " 'ihn',\n",
       " 'bar',\n",
       " 'siehe',\n",
       " 'microsoft',\n",
       " 'meine',\n",
       " '21',\n",
       " 'los',\n",
       " 'internet',\n",
       " '20',\n",
       " '00',\n",
       " 'sei',\n",
       " 'müssen',\n",
       " 'möglich',\n",
       " 'ihrem',\n",
       " '15',\n",
       " 'öffentlichen',\n",
       " 'ganz',\n",
       " 'ersten',\n",
       " 'erreichen',\n",
       " 'übernachten',\n",
       " 'täglich',\n",
       " 'schon',\n",
       " 'land',\n",
       " 'nachfolgend',\n",
       " 'jahre',\n",
       " 'Übernachtung',\n",
       " 'etwas',\n",
       " '6',\n",
       " 'updaterollup',\n",
       " 'neuen',\n",
       " 'jedoch',\n",
       " 'ja',\n",
       " 'icecat',\n",
       " 'große',\n",
       " 'diesen',\n",
       " 'dateien',\n",
       " 'd',\n",
       " 'benötigen',\n",
       " 'sollen',\n",
       " 'wieder',\n",
       " 'machen',\n",
       " 'gott',\n",
       " 'version',\n",
       " 'seite',\n",
       " 'frühstück',\n",
       " 'altstadt',\n",
       " '30',\n",
       " '—',\n",
       " 'platz',\n",
       " 'genießen',\n",
       " 'etc',\n",
       " '12',\n",
       " '”',\n",
       " 'wenige',\n",
       " 'weiter',\n",
       " 'preise',\n",
       " 'knecht',\n",
       " 'km',\n",
       " 'heute',\n",
       " 'bereits',\n",
       " 'kunden',\n",
       " 'joseph',\n",
       " 'system',\n",
       " 'märz',\n",
       " 'macht',\n",
       " 'flughafen',\n",
       " 'very',\n",
       " 'personen',\n",
       " 'herrn',\n",
       " 'großen',\n",
       " 'erhalten',\n",
       " 'aller',\n",
       " 'weitere',\n",
       " 'star',\n",
       " 'namen',\n",
       " 'meinem',\n",
       " 'gelegen',\n",
       " 'vielen',\n",
       " 'umgebung',\n",
       " 'i',\n",
       " 'dort',\n",
       " 'dir',\n",
       " 'dazu',\n",
       " 'website',\n",
       " 'unserer',\n",
       " 'daten',\n",
       " 'teil',\n",
       " 'kein',\n",
       " 'auswahl',\n",
       " 'atmosphäre',\n",
       " 'velázquez',\n",
       " 'restaurants',\n",
       " 'on',\n",
       " 'ob',\n",
       " 'national',\n",
       " 'zb',\n",
       " 'weil',\n",
       " 'viel',\n",
       " 'eigenen',\n",
       " 'dabei',\n",
       " 'bekannt',\n",
       " 'angeles',\n",
       " 'wlan',\n",
       " 'nutzen',\n",
       " 'gegeben',\n",
       " 'tage',\n",
       " 'neben',\n",
       " 'herunterladen',\n",
       " 'gute',\n",
       " 'datei',\n",
       " 'sowohl',\n",
       " 'meiner',\n",
       " 'geben',\n",
       " 'also',\n",
       " 'stadtzentrum',\n",
       " 'sollte',\n",
       " 'labview',\n",
       " 'dusseldorf',\n",
       " '18',\n",
       " 'programm',\n",
       " 'moderne',\n",
       " 'küche',\n",
       " 'kreditkarte',\n",
       " 'jeder',\n",
       " 'ideal',\n",
       " 'befinden',\n",
       " 'you',\n",
       " 'park',\n",
       " 'keinen',\n",
       " 'jahr',\n",
       " 'ferienwohnung',\n",
       " 'blick',\n",
       " 'basiert',\n",
       " 'angegebenen',\n",
       " '8',\n",
       " 'worden',\n",
       " 'verschiedene',\n",
       " 'sogar',\n",
       " 'drei',\n",
       " 'ausgestattet',\n",
       " '2008',\n",
       " '16',\n",
       " 'unternehmen',\n",
       " 'jedes',\n",
       " 'heiligen',\n",
       " 'geist',\n",
       " 'berlin',\n",
       " 'außerdem',\n",
       " '11',\n",
       " 'seinem',\n",
       " 'sehenswürdigkeiten',\n",
       " 'leben',\n",
       " 'kostenfrei',\n",
       " 'ihres',\n",
       " 'from',\n",
       " '®',\n",
       " 'underscore',\n",
       " 'sp4',\n",
       " 'problem',\n",
       " 'laterooms',\n",
       " 'komplettpreise',\n",
       " 'jetzt',\n",
       " 'gottes',\n",
       " '7',\n",
       " '14',\n",
       " 'unserem',\n",
       " 'erde',\n",
       " 'entwicklung',\n",
       " 'doch',\n",
       " 'aufenthalt',\n",
       " 'verschiedenen',\n",
       " 'spielen',\n",
       " 'sicherung',\n",
       " 'sage',\n",
       " 'personal',\n",
       " 'meinen',\n",
       " 'bereich',\n",
       " 'at',\n",
       " '24',\n",
       " 'wegen',\n",
       " 'unseren',\n",
       " 'ort',\n",
       " 'instruments',\n",
       " 'etwa',\n",
       " 'ebenfalls',\n",
       " 'besten',\n",
       " 'angebot',\n",
       " '100',\n",
       " 'zustellbetten',\n",
       " 'zahlreiche',\n",
       " 'your',\n",
       " 'wollen',\n",
       " 'statistik',\n",
       " 'parkplätze',\n",
       " 'muss',\n",
       " 'funktionen',\n",
       " 'ferienwohnungen',\n",
       " 'erwartet',\n",
       " 'ermöglicht',\n",
       " 'ecommerce',\n",
       " 'bzw',\n",
       " 'vergleichsseiten',\n",
       " 'teilnehmenden',\n",
       " 'modernen',\n",
       " 'garten',\n",
       " 'eshops',\n",
       " 'einkaufssysteme',\n",
       " 'ecommerceseiten',\n",
       " 'ebenso',\n",
       " 'distributoren',\n",
       " 'datenblatt',\n",
       " 'besuchen',\n",
       " 'asps',\n",
       " 'art',\n",
       " 'are',\n",
       " '91',\n",
       " 'zudem',\n",
       " 'weise',\n",
       " 'tun',\n",
       " 'schöne',\n",
       " 'schnell',\n",
       " 'probleme',\n",
       " 'per',\n",
       " 'nutzbar',\n",
       " 'möglichkeit',\n",
       " 'meer',\n",
       " 'information',\n",
       " 'hand',\n",
       " 'gebäude',\n",
       " 'ganze',\n",
       " 'e',\n",
       " 'besteht',\n",
       " 'besonders',\n",
       " 'anwendungen',\n",
       " 'this',\n",
       " 'terrasse',\n",
       " 'or',\n",
       " 'jeden',\n",
       " 'jedem',\n",
       " 'jede',\n",
       " 'oft',\n",
       " 'kostet',\n",
       " 'kommt',\n",
       " 'it',\n",
       " 'erste',\n",
       " 'ende',\n",
       " 'empfangen',\n",
       " 'dessen',\n",
       " 'beispiel',\n",
       " '93',\n",
       " '’',\n",
       " 'updates',\n",
       " 'seiten',\n",
       " 'pc',\n",
       " 'open',\n",
       " 'gemäß',\n",
       " 'gehen',\n",
       " 'entspannen',\n",
       " 'enthalten',\n",
       " 'dafür',\n",
       " 'ca',\n",
       " 'breakfast',\n",
       " 'bed',\n",
       " 'bahnhof',\n",
       " 'aufgrund',\n",
       " 'apartments',\n",
       " '19',\n",
       " 'volk',\n",
       " 'verwenden',\n",
       " 'sollten',\n",
       " 'ruhigen',\n",
       " 'nämlich',\n",
       " 'marx',\n",
       " 'house',\n",
       " 'gäste',\n",
       " 'gesamten',\n",
       " 'gehminuten',\n",
       " 'familie',\n",
       " 'fall',\n",
       " 'enthält',\n",
       " 'eignet',\n",
       " 'deren',\n",
       " 'darüber',\n",
       " '9',\n",
       " '17',\n",
       " 'we',\n",
       " 'villa',\n",
       " 'serviert',\n",
       " 'rooms',\n",
       " 'preis',\n",
       " 'neu',\n",
       " 'musik',\n",
       " 'kleine',\n",
       " 'ganzen',\n",
       " 'erstellen',\n",
       " 'double',\n",
       " 'design',\n",
       " 'davon',\n",
       " 'casino',\n",
       " 'arbeit',\n",
       " 'zimmern',\n",
       " 'vater',\n",
       " 'urlaub',\n",
       " 'stornierungen',\n",
       " 'stimme',\n",
       " 'privaten',\n",
       " 'mehrere',\n",
       " 'daher',\n",
       " 'wohnen',\n",
       " 'wer',\n",
       " 'stellt',\n",
       " 'online',\n",
       " 'nehmen',\n",
       " 'meisten',\n",
       " 'letzten',\n",
       " 'generation',\n",
       " 'findet',\n",
       " 'erfolgen',\n",
       " 'bequem',\n",
       " 'angebote',\n",
       " 'zeigt',\n",
       " 'wählen',\n",
       " 'will',\n",
       " 'vier',\n",
       " 'that',\n",
       " 'stil',\n",
       " 'server',\n",
       " 'private',\n",
       " 'new',\n",
       " 'kleinen',\n",
       " 'historischen',\n",
       " 'hatten',\n",
       " 'gegenüber',\n",
       " 'bereiche',\n",
       " '22',\n",
       " 'stunde',\n",
       " 'stellen',\n",
       " 'spricht',\n",
       " 'rund',\n",
       " 'prostitution',\n",
       " 'mitten',\n",
       " 'madrid',\n",
       " 'liegen',\n",
       " 'ins',\n",
       " 'indem',\n",
       " 'gilt',\n",
       " 'fragen',\n",
       " 'computer',\n",
       " 'bookingcom',\n",
       " 'anzahl',\n",
       " '25',\n",
       " '13',\n",
       " 'weltweit',\n",
       " 'weg',\n",
       " 'wahrlich',\n",
       " 'r',\n",
       " 'php',\n",
       " 'juni',\n",
       " 'installieren',\n",
       " 'insel',\n",
       " 'innerhalb',\n",
       " 'hauptbahnhof',\n",
       " 'fast',\n",
       " 'falls',\n",
       " 'diejenigen',\n",
       " 'deutschland',\n",
       " 'cala',\n",
       " 'best',\n",
       " 'zusammen',\n",
       " 'san',\n",
       " 'produkte',\n",
       " 'meines',\n",
       " 'leicht',\n",
       " 'k',\n",
       " 'f',\n",
       " 'el',\n",
       " 'bezug',\n",
       " 'as',\n",
       " 'allem',\n",
       " '50',\n",
       " '2006',\n",
       " 'zentral',\n",
       " 'suite',\n",
       " 'statt',\n",
       " 'sicher',\n",
       " 'seines',\n",
       " 'region',\n",
       " 'geschichte',\n",
       " 'genau',\n",
       " 'fahren',\n",
       " 'entwickelt',\n",
       " 'deine',\n",
       " 'bin',\n",
       " '2007',\n",
       " 'älteren',\n",
       " 'wäre',\n",
       " 'verwendet',\n",
       " 'tarifa',\n",
       " 'nichts',\n",
       " 'muß',\n",
       " 'le',\n",
       " 'kirche',\n",
       " 'hilfe',\n",
       " 'gt',\n",
       " 'gerade',\n",
       " 'gehören',\n",
       " 'führen',\n",
       " 'erst',\n",
       " 'beiden',\n",
       " 'bad',\n",
       " 'artikel',\n",
       " 'wert',\n",
       " 'usa',\n",
       " 'standard',\n",
       " 'nachdem',\n",
       " 'mobile',\n",
       " 'minute',\n",
       " 'kind',\n",
       " 'ideale',\n",
       " 'hin',\n",
       " 'gästen',\n",
       " 'gleich',\n",
       " 'fuß',\n",
       " 'erwachsene',\n",
       " 'en',\n",
       " 'einmal',\n",
       " 'dich',\n",
       " 'dank',\n",
       " 'würde',\n",
       " 'verfügen',\n",
       " 'verfügbar',\n",
       " 'us',\n",
       " 'suchen',\n",
       " 'stand',\n",
       " 'recht',\n",
       " 'modul',\n",
       " 'mithilfe',\n",
       " 'mal',\n",
       " 'küste',\n",
       " 'konnte',\n",
       " 'klicken',\n",
       " 'genutzt',\n",
       " 'gab',\n",
       " 'erlaubt',\n",
       " 'darauf',\n",
       " 'alten',\n",
       " 'unterstützt',\n",
       " 'unterkunft',\n",
       " 'tv',\n",
       " 'tool',\n",
       " 'stunden',\n",
       " 'starten',\n",
       " 'speisen',\n",
       " 'sicherheit',\n",
       " 'sagte',\n",
       " 'navy',\n",
       " 'meter',\n",
       " 'kostenfreien',\n",
       " 'je',\n",
       " 'jahrhundert',\n",
       " 'del',\n",
       " 'daran',\n",
       " '4sternehotel',\n",
       " 'zugang',\n",
       " 'wissen',\n",
       " 'staff',\n",
       " 'smith',\n",
       " 'schönen',\n",
       " 'schritte',\n",
       " 'not',\n",
       " 'liste',\n",
       " 'komfortable',\n",
       " 'komfort',\n",
       " 'installiert',\n",
       " 'gleichzeitig',\n",
       " 'folgenden',\n",
       " 'europa',\n",
       " 'einigen',\n",
       " 'dadurch',\n",
       " 'außer',\n",
       " 'aufpreis',\n",
       " 'anfang',\n",
       " '2005',\n",
       " 'webseite',\n",
       " 'verbindung',\n",
       " 'v',\n",
       " 'u',\n",
       " 'tropez',\n",
       " 'reisen',\n",
       " 'priestertum',\n",
       " 'natürlich',\n",
       " 'nachfrage',\n",
       " 'last',\n",
       " 'glauben',\n",
       " 'geboten',\n",
       " 'freuen',\n",
       " 'frage',\n",
       " 'empfängt',\n",
       " 'city',\n",
       " 'can',\n",
       " 'by',\n",
       " 'beschreibung',\n",
       " 'berühmten',\n",
       " 'ausflüge',\n",
       " 'arbeiten',\n",
       " 'allerdings',\n",
       " 'zeigen',\n",
       " 'willkommen',\n",
       " 'wichtigsten',\n",
       " 'weniger',\n",
       " 'suiten',\n",
       " 'spa',\n",
       " 'paris',\n",
       " 'nächsten',\n",
       " 'nie',\n",
       " 'licht',\n",
       " 'lesen',\n",
       " 'landes',\n",
       " 'karte',\n",
       " 'internationalen',\n",
       " 'internationale',\n",
       " 'inmitten',\n",
       " 'good',\n",
       " 'gemacht',\n",
       " 'fünf',\n",
       " 'funktion',\n",
       " 'form',\n",
       " 'dvd',\n",
       " 'cd',\n",
       " 'betten',\n",
       " '23',\n",
       " 'zwar',\n",
       " 'wenig',\n",
       " 'wasser',\n",
       " 'vorhandenen',\n",
       " 'verwendung',\n",
       " 'sprache',\n",
       " 'spiel',\n",
       " 'setzen',\n",
       " 'seien',\n",
       " 'prozent',\n",
       " 'nnen',\n",
       " 'möglichkeiten',\n",
       " 'london',\n",
       " 'links',\n",
       " 'hinweis',\n",
       " 'have',\n",
       " 'h',\n",
       " 'frau',\n",
       " 'firefox',\n",
       " 'euro',\n",
       " 'erforderlich',\n",
       " 'einzelnen',\n",
       " 'bringen',\n",
       " 'automatisch',\n",
       " '200',\n",
       " 'zurück',\n",
       " 'zahlreichen',\n",
       " 'stelle',\n",
       " 'st',\n",
       " 'rezeption',\n",
       " 'qualität',\n",
       " 'pool',\n",
       " 'one',\n",
       " 'könnte',\n",
       " 'insgesamt',\n",
       " 'hohen',\n",
       " 'halten',\n",
       " 'guten',\n",
       " 'geändert',\n",
       " 'gesprochen',\n",
       " 'email',\n",
       " 'einsatz',\n",
       " 'berechnet',\n",
       " 'benutzer',\n",
       " 'beispielsweise',\n",
       " 'beachten',\n",
       " 'apartment',\n",
       " 'ambiente',\n",
       " 'zeichen',\n",
       " 'zahlt',\n",
       " 'z',\n",
       " 'western',\n",
       " 'were',\n",
       " 'video',\n",
       " 'tagen',\n",
       " 'offenbarung',\n",
       " 'nichtraucherzonen',\n",
       " 'nichtraucherzimmer',\n",
       " 'mai',\n",
       " 'lässt',\n",
       " 'lange',\n",
       " 'karten',\n",
       " 'herz',\n",
       " 'gesehen',\n",
       " 'gerichte',\n",
       " 'gelangen',\n",
       " 'gehört',\n",
       " 'friendly',\n",
       " 'facilities',\n",
       " 'eingesetzt',\n",
       " 'download',\n",
       " 'but',\n",
       " 'bleiben',\n",
       " 'bilder',\n",
       " 'bevor',\n",
       " 'bestimmt',\n",
       " 'beide',\n",
       " 'bedingungen',\n",
       " 'ausstattung',\n",
       " 'aufzug',\n",
       " 'anzeigen',\n",
       " 'annehmlichkeiten',\n",
       " 'zweiten',\n",
       " 'zuletzt',\n",
       " 'zentrale',\n",
       " 'wohl',\n",
       " 'werke',\n",
       " 'weiteren',\n",
       " 'update',\n",
       " 'up',\n",
       " 'unterstützen',\n",
       " 'spanischen',\n",
       " 'sofort',\n",
       " 'sagen',\n",
       " 'ruhe',\n",
       " 'renovierte',\n",
       " 'propheten',\n",
       " 'plaza',\n",
       " 'location',\n",
       " 'kosten',\n",
       " 'klimaanlage',\n",
       " 'insbesondere',\n",
       " 'hinaus',\n",
       " 'hast',\n",
       " 'groß',\n",
       " 'geht',\n",
       " 'g',\n",
       " 'free',\n",
       " 'erstellung',\n",
       " 'erfahrung',\n",
       " 'darin',\n",
       " 'danach',\n",
       " 'club',\n",
       " 'center',\n",
       " 'business',\n",
       " 'bedeutet',\n",
       " 'beach',\n",
       " 'bars',\n",
       " 'außerhalb',\n",
       " 'anderer',\n",
       " 'aktuellen',\n",
       " 'zimmerservice',\n",
       " 'woche',\n",
       " 'welches',\n",
       " 'umgeben',\n",
       " 'text',\n",
       " 'teilen',\n",
       " 'systems',\n",
       " 'staaten',\n",
       " 'später',\n",
       " 'somit',\n",
       " 'sex',\n",
       " 'rahmen',\n",
       " 'platten',\n",
       " 'our',\n",
       " 'mehreren',\n",
       " 'markt',\n",
       " 'link',\n",
       " 'linie',\n",
       " 'leider',\n",
       " 'kunst',\n",
       " 'helfen',\n",
       " 'großer',\n",
       " 'gmbh',\n",
       " 'gefunden',\n",
       " 'familien',\n",
       " 'entdecken',\n",
       " 'dennoch',\n",
       " 'bus',\n",
       " 'bestimmten',\n",
       " 'besitzt',\n",
       " 'bedeutung',\n",
       " 'barcelona',\n",
       " 'worte',\n",
       " 'weiß',\n",
       " 'vision',\n",
       " 'via',\n",
       " 'use',\n",
       " 'unterkünfte',\n",
       " 'umfasst',\n",
       " 'tradition',\n",
       " 'support',\n",
       " 'suche',\n",
       " 'spiele',\n",
       " 'ruhiger',\n",
       " 'rechte',\n",
       " 'obwohl',\n",
       " 'museum',\n",
       " 'mitte',\n",
       " 'mindestens',\n",
       " 'leute',\n",
       " 'laden',\n",
       " 'kostenlos',\n",
       " 'komplett',\n",
       " 'jeweiligen',\n",
       " 'installation',\n",
       " 'html',\n",
       " 'himmel',\n",
       " 'geeignet',\n",
       " 'fällt',\n",
       " 'frühstücksbuffet',\n",
       " 'frauen',\n",
       " 'fax',\n",
       " 'erreichbar',\n",
       " 'erkunden',\n",
       " 'erbaut',\n",
       " 'entweder',\n",
       " 'einrichtungen',\n",
       " 'eingerichtet',\n",
       " 'dienstleistungen',\n",
       " 'costa',\n",
       " 'bucht',\n",
       " 'buch',\n",
       " 'browser',\n",
       " 'besucher',\n",
       " 'be',\n",
       " 'april',\n",
       " 'angeboten',\n",
       " 'xampp',\n",
       " 'wordpress',\n",
       " 'wobei',\n",
       " 'weit',\n",
       " 'wahl',\n",
       " 'vielzahl',\n",
       " 'veröffentlicht',\n",
       " 'verteilung',\n",
       " 'vaters',\n",
       " 'team',\n",
       " 'strände',\n",
       " 'sorgt',\n",
       " 'resort',\n",
       " 'reservierung',\n",
       " 'programme',\n",
       " 'pdf',\n",
       " 'natur',\n",
       " 'name',\n",
       " 'modern',\n",
       " 'lösungen',\n",
       " 'luxemburg',\n",
       " 'linux',\n",
       " 'kleines',\n",
       " 'hotelsafe',\n",
       " 'heizung',\n",
       " 'grund',\n",
       " 'google',\n",
       " 'gegend',\n",
       " 'freundlich',\n",
       " 'fotos',\n",
       " 'eure',\n",
       " 'euer',\n",
       " 'erfahren',\n",
       " 'darunter',\n",
       " 'code',\n",
       " 'bringt',\n",
       " 'besticht',\n",
       " 'benutzen',\n",
       " 'balkon',\n",
       " 'ausgangspunkt',\n",
       " 'anteile',\n",
       " 'anbindung',\n",
       " 'abendessen',\n",
       " '300',\n",
       " '24stundenrezeption',\n",
       " '€',\n",
       " 'zuvor',\n",
       " 'zug',\n",
       " 'ziel',\n",
       " 'zend',\n",
       " 'würden',\n",
       " 'wäscheservice',\n",
       " 'willen',\n",
       " 'werk',\n",
       " 'viertel',\n",
       " 'verbunden',\n",
       " 'solche',\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import os\n",
    "\n",
    "# using text vectorization\n",
    "# text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_fr = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_en.adapt(data[\"en\"])\n",
    "# text_vectorizer_fr.adapt(data[\"fr\"])\n",
    "\n",
    "en_vocabulary = []\n",
    "with open(os.path.join(\"./data/en-fr\", \"vocab.en\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        en_vocabulary.append(row.strip())\n",
    "\n",
    "de_vocabulary = []\n",
    "with open(\n",
    "    os.path.join(\"./data/en-de\", \"train_10k.de\"), \"r\", encoding=\"utf-8\"\n",
    ") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        de_vocabulary.append(row.strip())\n",
    "\n",
    "text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_de = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_en.adapt(en_vocabulary)\n",
    "text_vectorizer_de.adapt(de_vocabulary)\n",
    "\n",
    "\n",
    "en_vocabulary = text_vectorizer_en.get_vocabulary()\n",
    "de_vocabulary = text_vectorizer_de.get_vocabulary()\n",
    "text_vectorizer_de.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[UNK]', '[UNK]')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_unk_token = en_vocabulary.pop(1)\n",
    "de_unk_token = de_vocabulary.pop(1)\n",
    "\n",
    "en_unk_token, de_unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "en_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    oov_token=en_unk_token,\n",
    "    vocabulary=en_vocabulary,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")\n",
    "\n",
    "de_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    oov_token=de_unk_token,\n",
    "    vocabulary=de_vocabulary,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word IDs: [269792 395373 156726     75 275279 448899     75 287062      1 454356\n",
      "  94376 397282 159214      1]\n",
      "Sample vocabulary: ['<pad>', '[UNK]', '', 'o', 'av', 're', 'ms', 'm', 'i', 'd']\n"
     ]
    }
   ],
   "source": [
    "wid_sample = en_lookup_layer(\n",
    "    \"iron cement protects the ingot against the hot , abrasive steel casting process .\".split(\n",
    "        \" \"\n",
    "    )\n",
    ")\n",
    "print(f\"Word IDs: {wid_sample}\")\n",
    "print(f\"Sample vocabulary: {en_lookup_layer.get_vocabulary()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # Weights to compute Bahdanau attention\n",
    "        self.Wa = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.Ua = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention(use_scale=True)\n",
    "\n",
    "    def call(self, query, key, value, mask, return_attention_scores=False):\n",
    "\n",
    "        # Compute `Wa.ht`.\n",
    "        wa_query = self.Wa(query)\n",
    "\n",
    "        # Compute `Ua.hs`.\n",
    "        ua_key = self.Ua(key)\n",
    "\n",
    "        # Compute masks\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        # Compute the attention\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs=[wa_query, value, ua_key],\n",
    "            mask=[query_mask, value_mask, value_mask],\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "\n",
    "        if not return_attention_scores:\n",
    "            return context_vector\n",
    "        else:\n",
    "            return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">StringLookup</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">58,283,136</span> │ string_lookup_12… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ string_lookup_12… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">StringLookup</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)           │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,963,520</span> │ string_lookup_13… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ string_lookup_12… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bahdanau_attention  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BahdanauAttention</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)]   │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bahdanau_attenti… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30965</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">15,885,045</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mStringLookup\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │ \u001b[38;5;34m58,283,136\u001b[0m │ string_lookup_12… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ string_lookup_12… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mStringLookup\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)           │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m296,448\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m3,963,520\u001b[0m │ string_lookup_13… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ string_lookup_12… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m296,448\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bahdanau_attention  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m131,328\u001b[0m │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mBahdanauAttention\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m50\u001b[0m)]   │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bahdanau_attenti… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m30965\u001b[0m) │ \u001b[38;5;34m15,885,045\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,855,925</span> (300.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,855,925\u001b[0m (300.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,855,925</span> (300.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m78,855,925\u001b[0m (300.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Defining the encoder layers\n",
    "encoder_input = tf.keras.layers.Input(shape=(n_en_seq_length,), dtype=tf.string)\n",
    "# Converting tokens to IDs\n",
    "encoder_wid_out = en_lookup_layer(encoder_input)\n",
    "\n",
    "# Embedding layer and lookup\n",
    "encoder_emb_out = tf.keras.layers.Embedding(\n",
    "    len(en_lookup_layer.get_vocabulary()), 128, mask_zero=True\n",
    ")(encoder_wid_out)\n",
    "\n",
    "# Encoder GRU layer\n",
    "encoder_gru_out, encoder_gru_last_state = tf.keras.layers.GRU(\n",
    "    256, return_sequences=True, return_state=True\n",
    ")(encoder_emb_out)\n",
    "\n",
    "# Defining the encoder model: in - encoder_input / out - output of the GRU layer\n",
    "encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)\n",
    "\n",
    "# Defining the decoder layers\n",
    "decoder_input = tf.keras.layers.Input(shape=(n_de_seq_length - 1,), dtype=tf.string)\n",
    "# Converting tokens to IDs (Decoder)\n",
    "decoder_wid_out = de_lookup_layer(decoder_input)\n",
    "\n",
    "# Embedding layer and lookup (decoder)\n",
    "full_de_vocab_size = len(de_lookup_layer.get_vocabulary())\n",
    "decoder_emb_out = tf.keras.layers.Embedding(full_de_vocab_size, 128, mask_zero=True)(\n",
    "    decoder_wid_out\n",
    ")\n",
    "decoder_gru_out = tf.keras.layers.GRU(256, return_sequences=True)(\n",
    "    decoder_emb_out, initial_state=encoder_gru_last_state\n",
    ")\n",
    "\n",
    "# The attention mechanism (inputs: [q, v, k])\n",
    "decoder_attn_out, attn_weights = BahdanauAttention(256)(\n",
    "    query=decoder_gru_out,\n",
    "    key=encoder_gru_out,\n",
    "    value=encoder_gru_out,\n",
    "    mask=(encoder_wid_out != 0),\n",
    "    return_attention_scores=True,\n",
    ")\n",
    "\n",
    "# Concatenate GRU output and the attention output\n",
    "context_and_rnn_output = tf.keras.layers.Concatenate(axis=-1)(\n",
    "    [decoder_attn_out, decoder_gru_out]\n",
    ")\n",
    "\n",
    "# Final prediction layer (size of the vocabulary)\n",
    "decoder_out = tf.keras.layers.Dense(full_de_vocab_size, activation=\"softmax\")(\n",
    "    context_and_rnn_output\n",
    ")\n",
    "\n",
    "# Final seq2seq model\n",
    "seq2seq_model = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=decoder_out\n",
    ")\n",
    "\n",
    "# We will use this model later to visualize attention patterns\n",
    "attention_visualizer = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=[attn_weights, decoder_out]\n",
    ")\n",
    "\n",
    "# Compiling the model with a loss and an optimizer\n",
    "seq2seq_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:32: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:32: SyntaxWarning: invalid escape sequence '\\/'\n",
      "/tmp/ipykernel_29115/1856795582.py:32: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  \"<\\/s>.*\",\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "from bleu import compute_bleu\n",
    "\n",
    "\n",
    "class BLEUMetric(object):\n",
    "\n",
    "    def __init__(self, vocabulary, name=\"perplexity\", **kwargs):\n",
    "        \"\"\"Computes the BLEU score (Metric for machine translation)\"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab = vocabulary\n",
    "        self.id_to_token_layer = StringLookup(vocabulary=self.vocab, invert=True)\n",
    "\n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\"Calculate the BLEU score for targets and predictions\"\"\"\n",
    "\n",
    "        # Get the predicted token IDs\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)\n",
    "\n",
    "        # Convert token IDs to words using the vocabulary and the StringLookup\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
    "        real_tokens = self.id_to_token_layer(real)\n",
    "\n",
    "        def clean_text(tokens):\n",
    "            \"\"\"Clean padding and <s>/</s> tokens to only keep meaningful words\"\"\"\n",
    "\n",
    "            # 3. Strip the string of any extra white spaces\n",
    "            translations_in_bytes = tf.strings.strip(\n",
    "                # 2. Replace everything after the eos token with blank\n",
    "                tf.strings.regex_replace(\n",
    "                    # 1. Join all the tokens to one string in each sequence\n",
    "                    tf.strings.join(tf.transpose(tokens), separator=\" \"),\n",
    "                    \"<\\/s>.*\",\n",
    "                    \"\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Decode the byte stream to a string\n",
    "            translations = np.char.decode(\n",
    "                translations_in_bytes.numpy().astype(np.bytes_), encoding=\"utf-8\"\n",
    "            )\n",
    "\n",
    "            # If the string is empty, add a [UNK] token\n",
    "            # Otherwise get a Division by zero error\n",
    "            translations = [\n",
    "                sent if len(sent) > 0 else en_unk_token for sent in translations\n",
    "            ]\n",
    "\n",
    "            # Split the sequences to individual tokens\n",
    "            translations = np.char.split(translations).tolist()\n",
    "\n",
    "            return translations\n",
    "\n",
    "        # Get the clean versions of the predictions and real seuqences\n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n",
    "        real_tokens = [[token_seq] for token_seq in clean_text(real_tokens)]\n",
    "\n",
    "        # The compute_bleu method accpets the translations and references in the following format\n",
    "        # tranlation - list of list of tokens\n",
    "        # references - list of list of list of tokens\n",
    "        bleu, precisions, bp, ratio, translation_length, reference_length = (\n",
    "            compute_bleu(real_tokens, pred_tokens, smooth=False)\n",
    "        )\n",
    "\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score with longer correctly predicte phrases: 0.7598356856515925\n",
      "BLEU score without longer correctly predicte phrases: 0.537284965911771\n"
     ]
    }
   ],
   "source": [
    "translation = [\n",
    "    [\n",
    "        de_unk_token,\n",
    "        de_unk_token,\n",
    "        \"mÃssen\",\n",
    "        \"wir\",\n",
    "        \"in\",\n",
    "        \"erfahrung\",\n",
    "        \"bringen\",\n",
    "        \"wo\",\n",
    "        \"sie\",\n",
    "        \"wohnen\",\n",
    "    ]\n",
    "]\n",
    "reference = [\n",
    "    [\n",
    "        [\n",
    "            \"als\",\n",
    "            \"mÃssen\",\n",
    "            \"mÃssen\",\n",
    "            \"wir\",\n",
    "            \"in\",\n",
    "            \"erfahrung\",\n",
    "            \"bringen\",\n",
    "            \"wo\",\n",
    "            \"sie\",\n",
    "            \"wohnen\",\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "\n",
    "bleu1, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "translation = [\n",
    "    [\n",
    "        de_unk_token,\n",
    "        \"einmal\",\n",
    "        \"mÃssen\",\n",
    "        en_unk_token,\n",
    "        \"in\",\n",
    "        \"erfahrung\",\n",
    "        \"bringen\",\n",
    "        \"wo\",\n",
    "        \"sie\",\n",
    "        \"wohnen\",\n",
    "    ]\n",
    "]\n",
    "reference = [\n",
    "    [\n",
    "        [\n",
    "            \"als\",\n",
    "            \"mÃssen\",\n",
    "            \"mÃssen\",\n",
    "            \"wir\",\n",
    "            \"in\",\n",
    "            \"erfahrung\",\n",
    "            \"bringen\",\n",
    "            \"wo\",\n",
    "            \"sie\",\n",
    "            \"wohnen\",\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "bleu2, _, _, _, _, _ = compute_bleu(reference, translation)\n",
    "\n",
    "print(f\"BLEU score with longer correctly predicte phrases: {bleu1}\")\n",
    "print(f\"BLEU score without longer correctly predicte phrases: {bleu2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def prepare_data(de_lookup_layer, train_xy, valid_xy, test_xy):\n",
    "    \"\"\"Create a data dictionary from the dataframes containing data\"\"\"\n",
    "\n",
    "    data_dict = {}\n",
    "    for label, data_xy in zip(\n",
    "        [\"train\", \"valid\", \"test\"], [train_xy, valid_xy, test_xy]\n",
    "    ):\n",
    "\n",
    "        data_x, data_y = data_xy\n",
    "        en_inputs = data_x\n",
    "        de_inputs = data_y[:, :-1]\n",
    "        de_labels = de_lookup_layer(data_y[:, 1:]).numpy()\n",
    "        data_dict[label] = {\n",
    "            \"encoder_inputs\": en_inputs,\n",
    "            \"decoder_inputs\": de_inputs,\n",
    "            \"decoder_labels\": de_labels,\n",
    "        }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_inds=None):\n",
    "    \"\"\"Shuffle the data randomly (but all of inputs and labels at ones)\"\"\"\n",
    "\n",
    "    if shuffle_inds is None:\n",
    "        # If shuffle_inds are not passed create a shuffling automatically\n",
    "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        # Shuffle the provided shuffle_inds\n",
    "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
    "\n",
    "    # Return shuffled data\n",
    "    return (\n",
    "        en_inputs[shuffle_inds],\n",
    "        de_inputs[shuffle_inds],\n",
    "        de_labels[shuffle_inds],\n",
    "    ), shuffle_inds\n",
    "\n",
    "\n",
    "def check_for_nans(loss, model, en_lookup_layer, de_lookup_layer):\n",
    "\n",
    "    if np.isnan(loss):\n",
    "        for r_i in range(len(y)):\n",
    "            loss_sample, _ = model.evaluate(\n",
    "                [x[0][r_i : r_i + 1], x[1][r_i : r_i + 1]], y[r_i : r_i + 1], verbose=0\n",
    "            )\n",
    "            if np.isnan(loss_sample):\n",
    "\n",
    "                print(\"=\" * 25, \"nan detected\", \"=\" * 25)\n",
    "                print(\"train_batch\", i, \"r_i\", r_i)\n",
    "                print(\"en_input ->\", x[0][r_i].tolist())\n",
    "                print(\"en_input_wid ->\", en_lookup_layer(x[0][r_i]).numpy().tolist())\n",
    "                print(\"de_input ->\", x[1][r_i].tolist())\n",
    "                print(\"de_input_wid ->\", de_lookup_layer(x[1][r_i]).numpy().tolist())\n",
    "                print(\"de_output_wid ->\", y[r_i].tolist())\n",
    "\n",
    "                if r_i > 0:\n",
    "                    print(\"=\" * 25, \"no-nan\", \"=\" * 25)\n",
    "                    print(\"en_input ->\", x[0][r_i - 1].tolist())\n",
    "                    print(\n",
    "                        \"en_input_wid ->\",\n",
    "                        en_lookup_layer(x[0][r_i - 1]).numpy().tolist(),\n",
    "                    )\n",
    "                    print(\"de_input ->\", x[1][r_i - 1].tolist())\n",
    "                    print(\n",
    "                        \"de_input_wid ->\",\n",
    "                        de_lookup_layer(x[1][r_i - 1]).numpy().tolist(),\n",
    "                    )\n",
    "                    print(\"de_output_wid ->\", y[r_i - 1].tolist())\n",
    "                    return\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    en_lookup_layer,\n",
    "    de_lookup_layer,\n",
    "    train_xy,\n",
    "    valid_xy,\n",
    "    test_xy,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    predict_bleu_at_training=False,\n",
    "):\n",
    "    \"\"\"Training the model and evaluating on validation/test sets\"\"\"\n",
    "\n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "\n",
    "    # Define the data\n",
    "    data_dict = prepare_data(de_lookup_layer, train_xy, valid_xy, test_xy)\n",
    "\n",
    "    shuffle_inds = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Reset metric logs every epoch\n",
    "        if predict_bleu_at_training:\n",
    "            blue_log = []\n",
    "        accuracy_log = []\n",
    "        loss_log = []\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                         Train Phase                                 #\n",
    "        # =================================================================== #\n",
    "\n",
    "        # Shuffle data at the beginning of every epoch\n",
    "        if shuffle:\n",
    "            (en_inputs_raw, de_inputs_raw, de_labels), shuffle_inds = shuffle_data(\n",
    "                data_dict[\"train\"][\"encoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_labels\"],\n",
    "                shuffle_inds,\n",
    "            )\n",
    "        else:\n",
    "            (en_inputs_raw, de_inputs_raw, de_labels) = (\n",
    "                data_dict[\"train\"][\"encoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_labels\"],\n",
    "            )\n",
    "        # Get the number of training batches\n",
    "        n_train_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "        prev_loss = None\n",
    "        # Train one batch at a time\n",
    "        for i in range(n_train_batches):\n",
    "            # Status update\n",
    "            print(f\"Training batch {i+1}/{n_train_batches}\", end=\"\\r\")\n",
    "\n",
    "            # Get a batch of inputs (english and german sequences)\n",
    "            x = [\n",
    "                en_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "                de_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "            ]\n",
    "            # Get a batch of targets (german sequences offset by 1)\n",
    "            y = de_labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "\n",
    "            # Check if any samples are causing NaNs\n",
    "            check_for_nans(loss, model, en_lookup_layer, de_lookup_layer)\n",
    "\n",
    "            # Train for a single step\n",
    "            model.train_on_batch(x, y)\n",
    "            # Evaluate the model to get the metrics\n",
    "            # loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "\n",
    "            # Update the epoch's log records of the metrics\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "\n",
    "            if predict_bleu_at_training:\n",
    "                # Get the final prediction to compute BLEU\n",
    "                pred_y = model.predict(x)\n",
    "                bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "\n",
    "        print(\"\")\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        if predict_bleu_at_training:\n",
    "            print(\n",
    "                f\"\\t(train) loss: {np.mean(loss_log)} - accuracy: {np.mean(accuracy_log)} - bleu: {np.mean(bleu_log)}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"\\t(train) loss: {np.mean(loss_log)} - accuracy: {np.mean(accuracy_log)}\"\n",
    "            )\n",
    "        # =================================================================== #\n",
    "        #                      Validation Phase                               #\n",
    "        # =================================================================== #\n",
    "\n",
    "        val_en_inputs = data_dict[\"valid\"][\"encoder_inputs\"]\n",
    "        val_de_inputs = data_dict[\"valid\"][\"decoder_inputs\"]\n",
    "        val_de_labels = data_dict[\"valid\"][\"decoder_labels\"]\n",
    "\n",
    "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "            model,\n",
    "            de_lookup_layer,\n",
    "            val_en_inputs,\n",
    "            val_de_inputs,\n",
    "            val_de_labels,\n",
    "            batch_size,\n",
    "        )\n",
    "\n",
    "        # Print the evaluation metrics of each epoch\n",
    "        print(\n",
    "            f\"\\t(valid) loss: {val_loss} - accuracy: {val_accuracy} - bleu: {val_bleu}\"\n",
    "        )\n",
    "\n",
    "    # =================================================================== #\n",
    "    #                      Test Phase                                     #\n",
    "    # =================================================================== #\n",
    "\n",
    "    test_en_inputs = data_dict[\"test\"][\"encoder_inputs\"]\n",
    "    test_de_inputs = data_dict[\"test\"][\"decoder_inputs\"]\n",
    "    test_de_labels = data_dict[\"test\"][\"decoder_labels\"]\n",
    "\n",
    "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "        model,\n",
    "        de_lookup_layer,\n",
    "        test_en_inputs,\n",
    "        test_de_inputs,\n",
    "        test_de_labels,\n",
    "        batch_size,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n(test) loss: {test_loss} - accuracy: {test_accuracy} - bleu: {test_bleu}\")\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model, de_lookup_layer, en_inputs_raw, de_inputs_raw, de_labels, batch_size\n",
    "):\n",
    "    \"\"\"Evaluate the model on various metrics such as loss, accuracy and BLEU\"\"\"\n",
    "\n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "\n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    # Get the number of batches\n",
    "    n_batches = en_inputs_raw.shape[0] // batch_size\n",
    "    print(\" \", end=\"\\r\")\n",
    "\n",
    "    # Evaluate one batch at a time\n",
    "    for i in range(n_batches):\n",
    "        # Status update\n",
    "        print(f\"Evaluating batch {i+1}/{n_batches}\", end=\"\\r\")\n",
    "\n",
    "        # Get the inputs and targers\n",
    "        x = [\n",
    "            en_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "            de_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "        ]\n",
    "        y = de_labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "        # Get the evaluation metrics\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        # Get the predictions to compute BLEU\n",
    "        pred_y = model.predict(x)\n",
    "\n",
    "        # Update logs\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "\n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch 1/125\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: [['keras_tensor'], 'keras_tensor_6']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n",
      "W0000 00:00:1729010716.288403   29115 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"CPU\" vendor: \"GenuineIntel\" model: \"111\" frequency: 2303 num_cores: 24 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 49152 l2_cache_size: 1310720 l3_cache_size: 31457280 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2024-10-15 12:45:16.338459: E tensorflow/core/util/util.cc:131] oneDNN supports DT_BOOL only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "2024-10-15 12:45:16.447618: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [32,49,50] vs. [32,50]\n",
      "\t [[{{node functional_1_1/bahdanau_attention_1/additive_attention_1/sub}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node functional_1_1/bahdanau_attention_1/additive_attention_1/sub defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_29115/3462044664.py\", line 5, in <module>\n\n  File \"/tmp/ipykernel_29115/2909701450.py\", line 143, in train_model\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 432, in evaluate\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 165, in one_step_on_iterator\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in one_step_on_data\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 82, in test_step\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/models/functional.py\", line 597, in call\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/tmp/ipykernel_29115/2301211892.py\", line 23, in call\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/attention/attention.py\", line 229, in call\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/attention/attention.py\", line 177, in _apply_scores\n\nIncompatible shapes: [32,49,50] vs. [32,50]\n\t [[{{node functional_1_1/bahdanau_attention_1/additive_attention_1/sub}}]] [Op:__inference_one_step_on_iterator_3189]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m72\u001b[39m\n\u001b[1;32m      4\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq2seq_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43men_lookup_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mde_lookup_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_en_sentences_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_de_sentences_padded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_en_sentences_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_de_sentences_padded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_en_sentences_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_de_sentences_padded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt2\u001b[38;5;241m-\u001b[39mt1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds to complete the training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[50], line 143\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, en_lookup_layer, de_lookup_layer, train_xy, valid_xy, test_xy, epochs, batch_size, shuffle, predict_bleu_at_training)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Get a batch of targets (german sequences offset by 1)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m y \u001b[38;5;241m=\u001b[39m de_labels[i \u001b[38;5;241m*\u001b[39m batch_size : (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size]\n\u001b[0;32m--> 143\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Check if any samples are causing NaNs\u001b[39;00m\n\u001b[1;32m    146\u001b[0m check_for_nans(loss, model, en_lookup_layer, de_lookup_layer)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node functional_1_1/bahdanau_attention_1/additive_attention_1/sub defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_29115/3462044664.py\", line 5, in <module>\n\n  File \"/tmp/ipykernel_29115/2909701450.py\", line 143, in train_model\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 432, in evaluate\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 165, in one_step_on_iterator\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 154, in one_step_on_data\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 82, in test_step\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/models/functional.py\", line 182, in call\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/ops/function.py\", line 171, in _run_through_graph\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/models/functional.py\", line 597, in call\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/tmp/ipykernel_29115/2301211892.py\", line 23, in call\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 899, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 46, in __call__\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/attention/attention.py\", line 229, in call\n\n  File \"/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/attention/attention.py\", line 177, in _apply_scores\n\nIncompatible shapes: [32,49,50] vs. [32,50]\n\t [[{{node functional_1_1/bahdanau_attention_1/additive_attention_1/sub}}]] [Op:__inference_one_step_on_iterator_3189]"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 72\n",
    "\n",
    "t1 = time.time()\n",
    "train_model(\n",
    "    seq2seq_model,\n",
    "    en_lookup_layer,\n",
    "    de_lookup_layer,\n",
    "    (train_en_sentences_padded, train_de_sentences_padded),\n",
    "    (valid_en_sentences_padded, valid_de_sentences_padded),\n",
    "    (test_en_sentences_padded, test_de_sentences_padded),\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"\\nIt took {t2-t1} seconds to complete the training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_matrix_for_sampled_data(\n",
    "    attention_model, target_lookup_layer, test_xy, n_samples=5\n",
    "):\n",
    "\n",
    "    test_x, test_y = test_xy\n",
    "\n",
    "    rand_ids = np.random.randint(0, len(test_xy[0]), size=(n_samples,))\n",
    "    print(rand_ids)\n",
    "    results = []\n",
    "\n",
    "    for rid in rand_ids:\n",
    "        en_input = test_x[rid : rid + 1]\n",
    "        de_input = test_y[rid : rid + 1, :-1]\n",
    "\n",
    "        clean_en_input = []\n",
    "        en_start_i = 0\n",
    "        for i, w in enumerate(en_input.ravel()):\n",
    "            if w == \"<pad>\":\n",
    "                en_start_i = i + 1\n",
    "                continue\n",
    "\n",
    "            clean_en_input.append(w)\n",
    "            if w == \"</s>\":\n",
    "                break\n",
    "\n",
    "        attn_weights, predictions = attention_model.predict([en_input, de_input])\n",
    "        predicted_word_ids = np.argmax(predictions, axis=-1).ravel()\n",
    "        predicted_words = [\n",
    "            target_lookup_layer.get_vocabulary()[wid] for wid in predicted_word_ids\n",
    "        ]\n",
    "\n",
    "        clean_predicted_words = []\n",
    "        for w in predicted_words:\n",
    "            clean_predicted_words.append(w)\n",
    "            if w == \"</s>\":\n",
    "                break\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"attention_weights\": attn_weights[\n",
    "                    0,\n",
    "                    : len(clean_predicted_words),\n",
    "                    en_start_i : en_start_i + len(clean_en_input),\n",
    "                ],\n",
    "                \"input_words\": clean_en_input,\n",
    "                \"predicted_words\": clean_predicted_words,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# _, axes = plt.subplots(5, 1, figsize=(100,100))\n",
    "\n",
    "# attention_results = get_attention_matrix_for_sampled_data(\n",
    "#     attention_visualizer,\n",
    "#     de_lookup_layer,\n",
    "#     (test_en_sentences_padded, test_de_sentences_padded),\n",
    "#     n_samples = 5\n",
    "# )\n",
    "\n",
    "# for ax, result in zip(axes, attention_results):\n",
    "\n",
    "#     ax.imshow(result[\"attention_weights\"])\n",
    "#     x_labels = result[\"input_words\"]\n",
    "#     y_labels = result[\"predicted_words\"]\n",
    "#     ax.set_xticks(np.arange(len(x_labels)))\n",
    "#     ax.set_xticklabels(x_labels, rotation=45)\n",
    "#     ax.set_yticks(np.arange(len(y_labels)))\n",
    "#     ax.set_yticklabels(y_labels, rotation=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
