{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## The dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "the dataset is WMT-14 English-German translation data from https://nlp.stanford.edu/projects/nmt/. There over 4.5 million sentence pairs available. However, I will only use 10k pairs due to computational feasiblility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  The guests of the area are invited to stop at City Apart Hotel .\n",
      "German:  Die Gäste des Bereichs werden eingeladen , am City Apart Hotel zu stoppen . \n",
      "\n",
      "English:  The warehouse in the beginning is used for the storage of the spezie and other articles of commerce of the Far East .\n",
      "German:  Das Lager am Anfang wird für die Lagerung des spezie und anderer Artikel des Handels vom Fernen Osten benutzt . \n",
      "\n",
      "English:  You can chose from a wide selection of over 60 of Casino Tropez � s most popular games , and play either for real money or in practice mode .\n",
      "German:  Sie k � nnen sich zwischen 60 verschiedenen Casino Tropez Spielen entscheiden und sogar um echtes Geld spielen . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "n_sentences = 10000\n",
    "\n",
    "# Loading English train sentences\n",
    "original_en_sentences = []\n",
    "with open(os.path.join(\"data\", \"train_10k.en\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for i, row in enumerate(en_file):\n",
    "        if i >= n_sentences:\n",
    "            break\n",
    "        original_en_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "# loading German train sentences\n",
    "original_de_sentences = []\n",
    "with open(os.path.join(\"data\", \"train_10k.de\"), \"r\", encoding=\"utf-8\") as de_file:\n",
    "    for i, row in enumerate(de_file):\n",
    "        if i >= n_sentences:\n",
    "            break\n",
    "        original_de_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "# Loading English test sentences\n",
    "oritinal_en_test_sentences = []\n",
    "\n",
    "with open(os.path.join(\"data\", \"test_100.en\"), \"r\", encoding=\"utf-8\") as de_file:\n",
    "    for i, row in enumerate(de_file):\n",
    "        if i >= n_sentences:\n",
    "            break\n",
    "        oritinal_en_test_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "# Loading German test sentences\n",
    "oritinal_de_test_sentences = []\n",
    "with open(os.path.join(\"data\", \"test_100.de\"), \"r\", encoding=\"utf-8\") as de_file:\n",
    "    for i, row in enumerate(de_file):\n",
    "        if i >= n_sentences:\n",
    "            break\n",
    "        oritinal_de_test_sentences.append(row.strip().split(\" \"))\n",
    "\n",
    "### displaying random sentences and their respective translations\n",
    "for i in range(3):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(original_en_sentences[index]))\n",
    "    print(\"German: \", \" \".join(original_de_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding special tokens\n",
    "\n",
    "#### I will add \"< s >\" to mark the start of a sentence and \"< /s >\" to mark the end of a sentence\n",
    "\n",
    "This way\n",
    "we prediction can be done for an arbitrary number of time steps. Using < s > as the starting token gives a\n",
    "way to signal to the decoder that it should start predicting tokens from the target language.\n",
    "\n",
    "if < /s > token is not used to mark the end of a sentence, the decoder cannot be signaled to\n",
    "end a sentence. This can lead the model to enter an infinite loop of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  <s> In these portraits , Velázquez has well repaid the debt of gratitude that he owed to his first patron , whom Velázquez stood by during Olivares &apos;s fall from power , thus exposing himself to the great risk of the anger of the jealous Philip . The king , however , showed no sign of malice towards his favorite painter . </s>\n",
      "German:  <s> Während dieser Reise muss Velázquez Details der Übergabe von Breda von dem damaligen Kommandanten des spanischen Heeres gehört und eine erste Porträtstudie für sein späteres Historiengemälde der Übergabe von Breda gemacht haben . </s> \n",
      "\n",
      "English:  <s> � � El Gordo � works differently to most lottery games played , as countless people can share one single ticket . </s>\n",
      "German:  <s> &quot; El Gordo &quot; verl � uft anders als die meisten Lotterie Spiele . </s> \n",
      "\n",
      "English Test:  <s> Orlando Bloom and Miranda Kerr still love each other </s>\n",
      "German Test:  <s> Orlando Bloom und Miranda Kerr lieben sich noch immer </s>\n"
     ]
    }
   ],
   "source": [
    "en_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_en_sentences]\n",
    "de_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_de_sentences]\n",
    "test_en_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in oritinal_en_test_sentences]\n",
    "test_de_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in oritinal_de_test_sentences]\n",
    "\n",
    "for i in range(2):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(en_sentences[index]))\n",
    "    print(\"German: \", \" \".join(de_sentences[index]), \"\\n\")\n",
    "\n",
    "print(\"English Test: \", \" \".join(test_en_sentences[0]))\n",
    "print(\"German Test: \", \" \".join(test_de_sentences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting training and validation dataset\n",
    "\n",
    "#### 90% training and 10% validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'While', 'access', 'to', 'software', 'determines', 'our', 'ability', 'to', 'participate', 'in', 'a', 'digital', 'society', 'and', 'governs', 'our', 'ability', 'for', 'communication', ',', 'education', 'and', 'work', ',', 'software', 'itself', 'represents', 'a', 'reservoir', 'of', 'codified', 'skill', '.', '</s>']\n",
      "['<s>', 'Während', 'der', 'Zugang', 'zu', 'Software', 'über', 'unsere', 'Fähigkeit', 'entscheidet', ',', 'an', 'der', 'digitalen', 'Gesellschaft', 'teilhaben', 'zu', 'können', 'und', 'unsere', 'Möglichkeiten', 'bei', 'Kommunikation', ',', 'Bildung', 'und', 'Beruf', 'entscheidend', 'bestimmt', ',', 'stellt', 'Software', 'gleichzeitig', 'auch', 'ein', 'Reservoir', 'von', 'in', 'Programmcode', 'gegossenen', 'Fertigkeiten', 'dar', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "train_en_sentences, valid_en_sentences, train_de_sentences, valid_de_sentences = (\n",
    "    train_test_split(en_sentences, de_sentences, test_size=0.1)\n",
    ")\n",
    "\n",
    "print(train_en_sentences[1])\n",
    "print(train_de_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining sequence leghts fot he two languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9000.000000\n",
       "mean       27.364222\n",
       "std        14.288396\n",
       "min         8.000000\n",
       "5%         11.000000\n",
       "50%        24.000000\n",
       "95%        56.000000\n",
       "max       102.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Getting some basic statistics from the data\n",
    "\n",
    "# convert train_en_sentences to a pandas series\n",
    "pd.Series(train_en_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
