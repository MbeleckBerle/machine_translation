{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# The data\n",
    "\n",
    "https://www.kaggle.com/datasets/mohamedlotfy50/wmt-2014-english-french/data\n",
    "There over 4.5 million sentence pairs available. However, I will only use 25,000 pairs due to computational feasiblility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Reprise de la session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Je déclare reprise la session du Parlement eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Comme vous avez pu le constater, le grand \"bog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Vous avez souhaité un débat à ce sujet dans le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>En attendant, je souhaiterais, comme un certai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                                  fr  \n",
       "0                              Reprise de la session  \n",
       "1  Je déclare reprise la session du Parlement eur...  \n",
       "2  Comme vous avez pu le constater, le grand \"bog...  \n",
       "3  Vous avez souhaité un débat à ce sujet dans le...  \n",
       "4  En attendant, je souhaiterais, comme un certai...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_sentences = 25000\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"./data/en-fr/wmt14_translate_fr-en_train.csv\", nrows=n_sentences\n",
    ").dropna()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spliting the sentences into tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  Therefore, in future, prior approval should take place in a targeted manner, that is, only in cases of uncertainty or risk.\n",
      "French:  Les fonctionnaires chargés du contrôle financier devraient exercer leurs fonctions de manière décentralisée, c' est-à-dire dans les directions générales, auprès des collègues qui dépensent l' argent. \n",
      "\n",
      "English:  As regards the other aspect of Agenda, the aspect of cohesion and regional development, there we do indeed have great achievements to point to, but there are still less developed regions, particularly island regions, to which more attention should be paid.\n",
      "French:  En ce qui concerne l' autre volet de l' Agenda, Monsieur le Président de la Commission, à savoir l'aspect de la cohésion et du développement régional, il y a, certes, de grands succès, cependant, il subsiste encore des régions qui restent en retard, et notamment les régions insulaires, sur lesquelles nous devrions nous attarder. \n",
      "\n",
      "English:  I do not need to remind you that consumer confidence in Europe's food safety regime has been badly affected by the crises and alerts of recent years and months.\n",
      "French:  Je n'ai pas besoin de vous rappeler que la confiance des consommateurs dans le régime de sécurité alimentaire de l'Europe a été sévèrement affectée par les crises et alertes de ces dernières années et de ces derniers mois. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "original_en_sentences = [sent.strip().split(\" \") for sent in data[\"en\"]]\n",
    "original_fr_sentences = [sent.strip().split(\" \") for sent in data[\"fr\"]]\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(original_en_sentences[index]))\n",
    "    print(\"French: \", \" \".join(original_fr_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding special tokens\n",
    "\n",
    "#### I will add \"< s >\" to mark the start of a sentence and \"< /s >\" to mark the end of a sentence\n",
    "\n",
    "This way\n",
    "we prediction can be done for an arbitrary number of time steps. Using < s > as the starting token gives a\n",
    "way to signal to the decoder that it should start predicting tokens from the target language.\n",
    "\n",
    "if < /s > token is not used to mark the end of a sentence, the decoder cannot be signaled to\n",
    "end a sentence. This can lead the model to enter an infinite loop of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  <s> Finally, I would like to thank Parliament for the very constructive debate on the key aspects of these proposals and, in particular, of course, the rapporteur, Mrs Berger. </s>\n",
      "German:  <s> Enfin, je voudrais remercier le Parlement pour le débat très constructif sur les aspects fondamentaux de ces propositions et, en particulier, bien sûr, le rapporteur, Mme Berger. </s> \n",
      "\n",
      "English:  <s> Mr Kinnock has also devoted some fine-sounding phrases to this subject but, at the same time, it is entirely unclear, at this moment when we have to make a decision, what, for example, happens with whistle-blowers who want to get something off their chest and cannot do this internally but who want to address the outside world - the press or Parliament. </s>\n",
      "German:  <s> M. Kinnock y a consacré de belles paroles mais, en même temps, il n' est toujours pas clair du tout, au moment où nous devons prendre une décision, de savoir ce qui va se passer lorsque ces informateurs ne trouveront pas d' interlocuteur à l' intérieur de l' institution et voudront s' adresser à l' extérieur, à la presse, au Parlement. </s> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_en_sentences]\n",
    "fr_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_fr_sentences]\n",
    "\n",
    "for i in range(2):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(en_sentences[index]))\n",
    "    print(\"German: \", \" \".join(fr_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting training and validation dataset\n",
    "\n",
    "#### 80% training, 10% validation and 10% for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'We', 'can', 'only', 'do', 'that', 'by', 'intensifying', 'co-operation', 'within', 'the', 'Community.', '</s>']\n",
      "['<s>', 'Ce', 'ne', 'sera', 'possible', 'que', 'par', 'le', 'biais', \"d'un\", 'renforcement', 'de', 'la', 'coopération', 'au', 'sein', 'de', 'la', 'Communauté.', '</s>']\n",
      "\n",
      "\n",
      "['<s>', 'One', 'of', 'the', 'very', 'first', 'speakers,', 'Mr', 'Wuori,', 'spoke', 'of', 'the', 'remarks', 'made', 'earlier', 'today', 'by', 'Mr', 'Havel.', '</s>']\n",
      "['<s>', 'Un', 'des', 'tout', 'premiers', 'orateurs,', 'M.', 'Wuori,', 'a', 'parlé', 'des', 'remarques', \"qu'a\", 'faites', \"aujourd'hui\", 'M.', 'Havel.', '</s>']\n",
      "Train size: 20000\n",
      "Valid size: 2500\n",
      "Test size: 2500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "(\n",
    "    train_en_sentences,\n",
    "    valid_test_en_sentences,\n",
    "    train_fr_sentences,\n",
    "    valid_test_fr_sentences,\n",
    ") = train_test_split(en_sentences, fr_sentences, test_size=0.2)\n",
    "\n",
    "\n",
    "(valid_en_sentences, test_en_sentences, valid_fr_sentences, test_fr_sentences) = (\n",
    "    train_test_split(valid_test_en_sentences, valid_test_fr_sentences, test_size=0.5)\n",
    ")\n",
    "\n",
    "\n",
    "print(train_en_sentences[1])\n",
    "print(train_fr_sentences[1])\n",
    "print(\"\\n\")\n",
    "print(test_en_sentences[0])\n",
    "print(test_fr_sentences[0])\n",
    "\n",
    "print(f\"Train size: {len(train_en_sentences)}\")\n",
    "print(f\"Valid size: {len(valid_en_sentences)}\")\n",
    "print(f\"Test size: {len(test_en_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining sequence leghts fot the two languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        27.589950\n",
       "std         15.737363\n",
       "min          3.000000\n",
       "5%           8.000000\n",
       "50%         24.000000\n",
       "95%         57.000000\n",
       "max        150.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting some basic statistics from the data\n",
    "\n",
    "# convert train_en_sentences to a pandas series\n",
    "pd.Series(train_en_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        28.874700\n",
       "std         16.748511\n",
       "min          3.000000\n",
       "5%           8.000000\n",
       "50%         26.000000\n",
       "95%         60.000000\n",
       "max        148.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_fr_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from the train data statistics above, 95% of the english sentences have lengths of 57 while in the french, 95 % of sentences have lengths of 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding the sentences with pad_sequences from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>' 'We' 'can' 'only' 'do' 'that' 'by' 'intensifying' 'co-operation'\n",
      " 'within' 'the' 'Community.' '</s>' '[pad]' '[pad]' '[pad]' '[pad]'\n",
      " '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]'\n",
      " '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]'\n",
      " '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]'\n",
      " '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]'\n",
      " '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "n_en_seq_length = 60\n",
    "n_fr_seq_length = 60\n",
    "# unk_token = \"<unk>\"\n",
    "pad_token = \"[pad]\"\n",
    "\n",
    "train_en_sentences_padded = pad_sequences(\n",
    "    train_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_en_sentences_padded = pad_sequences(\n",
    "    valid_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_en_sentences_padded = pad_sequences(\n",
    "    test_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "\n",
    "train_fr_sentences_padded = pad_sequences(\n",
    "    train_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_fr_sentences_padded = pad_sequences(\n",
    "    valid_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_fr_sentences_padded = pad_sequences(\n",
    "    test_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "print(train_en_sentences_padded[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import os\n",
    "\n",
    "# using text vectorization\n",
    "# text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_fr = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_en.adapt(data[\"en\"])\n",
    "# text_vectorizer_fr.adapt(data[\"fr\"])\n",
    "\n",
    "en_vocabulary = []\n",
    "with open(os.path.join(\"./data/en-fr\", \"vocab.en\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        en_vocabulary.append(row.strip())\n",
    "\n",
    "fr_vocabulary = []\n",
    "with open(os.path.join(\"./data/en-fr\", \"vocab.fr\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        fr_vocabulary.append(row.strip())\n",
    "\n",
    "text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_fr = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_en.adapt(en_vocabulary)\n",
    "text_vectorizer_fr.adapt(fr_vocabulary)\n",
    "\n",
    "\n",
    "en_vocabulary = text_vectorizer_en.get_vocabulary()\n",
    "fr_vocabulary = text_vectorizer_fr.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[UNK]', '[UNK]')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_unk_token = en_vocabulary.pop(1)\n",
    "fr_unk_token = fr_vocabulary.pop(1)\n",
    "\n",
    "en_unk_token, fr_unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# pad_token = \"<PAD>\"\n",
    "\n",
    "# English look up layer\n",
    "en_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=en_vocabulary,\n",
    "    oov_token=en_unk_token,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")\n",
    "\n",
    "# French look up layer\n",
    "fr_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=fr_vocabulary,\n",
    "    oov_token=en_unk_token,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word IDs: [269792 395373 156726     75 275279 448899     75 287062      1 454356\n",
      "  94376 397282 159214      1]\n",
      "Sample vocabulary: ['[pad]', '[UNK]', '', 'o', 'av', 're', 'ms', 'm', 'i', 'd']\n"
     ]
    }
   ],
   "source": [
    "wid_sample = en_lookup_layer(\n",
    "    \"iron cement protects the ingot against the hot , abrasive steel casting process .\".split(\n",
    "        \" \"\n",
    "    )\n",
    ")\n",
    "print(f\"Word IDs: {wid_sample}\")\n",
    "print(f\"Sample vocabulary: {en_lookup_layer.get_vocabulary()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(en_lookup_layer)\n",
    "# en_lookup_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes n_en_seq_length of sentences\n",
    "encoder_input = tf.keras.layers.Input(shape=(n_en_seq_length,), dtype=tf.string)\n",
    "\n",
    "# using lookup layer into word IDs\n",
    "encoder_wid_out = en_lookup_layer(encoder_input)\n",
    "\n",
    "\"\"\"\n",
    "With the tokens converted into IDs, route the generated word IDs to a token embedding layer.\n",
    "Pass in the size of the vocabulary (derived from the en_lookup_layer's get_vocabulary()\n",
    "method) and the embedding size (128) and finally then ask the layer to mask any zero-valued inputs\n",
    "as they don’t contain any information:\n",
    "\n",
    "\"\"\"\n",
    "en_full_vocab_size = len(en_lookup_layer.get_vocabulary())\n",
    "encoder_emb_out = tf.keras.layers.Embedding(en_full_vocab_size, 128, mask_zero=True)(\n",
    "    encoder_wid_out\n",
    ")\n",
    "\n",
    "\n",
    "encoder_gru_out, encoder_gru_last_state = tf.keras.layers.GRU(\n",
    "    256, return_sequences=True, return_state=True\n",
    ")(encoder_emb_out)\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Decoder with teacher forcing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = tf.keras.layers.Input(shape=(n_fr_seq_length - 1,), dtype=tf.string)\n",
    "\n",
    "# convert tokens to IDs using the de_lookup_layer\n",
    "decoder_wid_out = fr_lookup_layer(decoder_input)\n",
    "\n",
    "# decoder embedding layer\n",
    "fr_full_vocab_size = len(fr_lookup_layer.get_vocabulary())\n",
    "decoder_emb_out = tf.keras.layers.Embedding(fr_full_vocab_size, 128, mask_zero=True)(\n",
    "    decoder_wid_out\n",
    ")\n",
    "\n",
    "# decoder layer>>> pass the last state of the encoder into the decoder\n",
    "decoder_gru_out = tf.keras.layers.GRU(256, return_sequences=True)(\n",
    "    decoder_emb_out, initial_state=encoder_gru_last_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Badanau Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # Weights to compute Bahdanau attention\n",
    "        self.Wa = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.Ua = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention(use_scale=True)\n",
    "\n",
    "    def call(self, query, key, value, mask, return_attention_scores=False):\n",
    "\n",
    "        # Compute `Wa.ht`.\n",
    "        wa_query = self.Wa(query)\n",
    "\n",
    "        # Compute `Ua.hs`.\n",
    "        ua_key = self.Ua(key)\n",
    "\n",
    "        # Compute masks\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        # Compute the attention\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs=[wa_query, value, ua_key],\n",
    "            mask=[query_mask, value_mask, value_mask],\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "\n",
    "        if not return_attention_scores:\n",
    "            return context_vector\n",
    "        else:\n",
    "            return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'bahdanau_attention' (of type BahdanauAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">StringLookup</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">58,283,136</span> │ string_lookup_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ string_lookup_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">StringLookup</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)           │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,792,832</span> │ string_lookup_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ string_lookup_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bahdanau_attention  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BahdanauAttention</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)]   │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bahdanau_attenti… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21819</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">11,193,147</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mStringLookup\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │ \u001b[38;5;34m58,283,136\u001b[0m │ string_lookup_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ string_lookup_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mStringLookup\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)           │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m296,448\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m2,792,832\u001b[0m │ string_lookup_3[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ string_lookup_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m296,448\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bahdanau_attention  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m131,328\u001b[0m │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mBahdanauAttention\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m60\u001b[0m)]   │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bahdanau_attenti… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m21819\u001b[0m) │ \u001b[38;5;34m11,193,147\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72,993,339</span> (278.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m72,993,339\u001b[0m (278.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72,993,339</span> (278.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m72,993,339\u001b[0m (278.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# Defining the encoder layers\n",
    "encoder_input = tf.keras.layers.Input(shape=(n_en_seq_length,), dtype=tf.string)\n",
    "# Converting tokens to IDs\n",
    "encoder_wid_out = en_lookup_layer(encoder_input)\n",
    "\n",
    "# Embedding layer and lookup\n",
    "encoder_emb_out = tf.keras.layers.Embedding(\n",
    "    len(en_lookup_layer.get_vocabulary()), 128, mask_zero=True\n",
    ")(encoder_wid_out)\n",
    "\n",
    "# Encoder GRU layer\n",
    "encoder_gru_out, encoder_gru_last_state = tf.keras.layers.GRU(\n",
    "    256, return_sequences=True, return_state=True\n",
    ")(encoder_emb_out)\n",
    "\n",
    "# Defining the encoder model: in - encoder_input / out - output of the GRU layer\n",
    "encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)\n",
    "\n",
    "# Defining the decoder layers\n",
    "decoder_input = tf.keras.layers.Input(shape=(n_fr_seq_length - 1,), dtype=tf.string)\n",
    "# Converting tokens to IDs (Decoder)\n",
    "decoder_wid_out = fr_lookup_layer(decoder_input)\n",
    "\n",
    "# Embedding layer and lookup (decoder)\n",
    "full_de_vocab_size = len(fr_lookup_layer.get_vocabulary())\n",
    "decoder_emb_out = tf.keras.layers.Embedding(full_de_vocab_size, 128, mask_zero=True)(\n",
    "    decoder_wid_out\n",
    ")\n",
    "decoder_gru_out = tf.keras.layers.GRU(256, return_sequences=True)(\n",
    "    decoder_emb_out, initial_state=encoder_gru_last_state\n",
    ")\n",
    "\n",
    "# The attention mechanism (inputs: [q, v, k])\n",
    "decoder_attn_out, attn_weights = BahdanauAttention(256)(\n",
    "    query=decoder_gru_out,\n",
    "    key=encoder_gru_out,\n",
    "    value=encoder_gru_out,\n",
    "    mask=(encoder_wid_out != 0),\n",
    "    return_attention_scores=True,\n",
    ")\n",
    "\n",
    "# Concatenate GRU output and the attention output\n",
    "context_and_rnn_output = tf.keras.layers.Concatenate(axis=-1)(\n",
    "    [decoder_attn_out, decoder_gru_out]\n",
    ")\n",
    "\n",
    "# Final prediction layer (size of the vocabulary)\n",
    "decoder_out = tf.keras.layers.Dense(full_de_vocab_size, activation=\"softmax\")(\n",
    "    context_and_rnn_output\n",
    ")\n",
    "\n",
    "# Final seq2seq model\n",
    "seq2seq_model = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=decoder_out\n",
    ")\n",
    "\n",
    "# We will use this model later to visualize attention patterns\n",
    "attention_visualizer = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=[attn_weights, decoder_out]\n",
    ")\n",
    "\n",
    "# Compiling the model with a loss and an optimizer\n",
    "seq2seq_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'bahdanau_attention_1' (of type BahdanauAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "decoder_attn_out, attn_weights = BahdanauAttention(256)(\n",
    "    query=decoder_gru_out,\n",
    "    key=encoder_gru_out,\n",
    "    value=encoder_gru_out,\n",
    "    mask=(encoder_wid_out != 0),  # mask that denotes which tokens need to be ignored\n",
    "    return_attention_scores=True,\n",
    ")\n",
    "\n",
    "\n",
    "# combine the attention output and the decoder's GRU output to create a\n",
    "# single concatenated input for the prediction\n",
    "context_and_gru_output = tf.keras.layers.Concatenate(axis=-1)(\n",
    "    [decoder_attn_out, decoder_gru_out]\n",
    ")\n",
    "\n",
    "# Prediction layer takes the concatenated attention's context vewctore andthe GRU ouput to\n",
    "# produce probability distributions over the French tokens for each time step\n",
    "decoder_out = tf.keras.layers.Dense(fr_full_vocab_size, activation=\"softmax\")(\n",
    "    context_and_gru_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final end-to-end model\n",
    "seq2seq_model = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=decoder_out\n",
    ")\n",
    "\n",
    "seq2seq_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_visualizer = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=[attn_weights, decoder_out]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(fr_lookup_layer, train_xy, valid_xy, test_xy):\n",
    "    \"\"\"\n",
    "    The prepare_data() function takes the source sentence and target sentence pairs and generates\n",
    "    encoder and decoder inputs and decoder labels.\n",
    "\n",
    "    • fr_lookup_layer   =>  The StringLookup layer of the French language\n",
    "    • train_xy          =>  A tuple containing tokenized English sentences and tokenized\n",
    "                            French sentences in the training set, respectively\n",
    "    • valid_xy          =>  Similar to train_xy but for validation data\n",
    "    • test_xy           =>  Similar to train_xy but for test data\n",
    "\n",
    "\n",
    "    For each training, validation, and test dataset, this function generates:\n",
    "\n",
    "    • encoder_inputs => Tokenized English sentences as in the preprocessed dataset\n",
    "    • decoder_inputs => All tokens except the last of each French sentence\n",
    "    • decoder_labels => All token IDs except the first of each French sentence, where token\n",
    "                        IDs are generated by the fr_lookup_layer\n",
    "\n",
    "    decoder_labels will be decoder_inputs shifted one token to the left\n",
    "    \"\"\"\n",
    "    # Create a data dictionary from the dataframes containing data\n",
    "    data_dict = {}\n",
    "    for label, data_xy in zip(\n",
    "        [\"train\", \"valid\", \"test\"], [train_xy, valid_xy, test_xy]\n",
    "    ):\n",
    "        data_x, data_y = data_xy\n",
    "        en_inputs = data_x\n",
    "        fr_inputs = data_y[:, :-1]\n",
    "        fr_labels = fr_lookup_layer(data_y[:, 1:]).numpy()\n",
    "        data_dict[label] = {\n",
    "            \"encoder_input\": en_inputs,\n",
    "            \"decoder_inputs\": fr_inputs,\n",
    "            \"decoder_labels\": fr_labels,\n",
    "        }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(en_inputs, fr_inputs, fr_labels, shuffle_inds=None):\n",
    "    \"\"\"\n",
    "    Shuffle the data randomly (but all inputs and labels at once)\n",
    "    \"\"\"\n",
    "\n",
    "    if shuffle_inds is None:\n",
    "        # If shuffle_inds are not passed, create a shuffle automatically\n",
    "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        # Shuffle the provided shuffle_inds\n",
    "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
    "\n",
    "    # return shuffled data\n",
    "    return (\n",
    "        en_inputs[shuffle_inds],\n",
    "        fr_inputs[shuffle_inds],\n",
    "        fr_labels[shuffle_inds],\n",
    "    ), shuffle_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def prepare_data(de_lookup_layer, train_xy, valid_xy, test_xy):\n",
    "    \"\"\"Create a data dictionary from the dataframes containing data\"\"\"\n",
    "\n",
    "    data_dict = {}\n",
    "    for label, data_xy in zip(\n",
    "        [\"train\", \"valid\", \"test\"], [train_xy, valid_xy, test_xy]\n",
    "    ):\n",
    "\n",
    "        data_x, data_y = data_xy\n",
    "        en_inputs = data_x\n",
    "        de_inputs = data_y[:, :-1]\n",
    "        de_labels = de_lookup_layer(data_y[:, 1:]).numpy()\n",
    "        data_dict[label] = {\n",
    "            \"encoder_inputs\": en_inputs,\n",
    "            \"decoder_inputs\": de_inputs,\n",
    "            \"decoder_labels\": de_labels,\n",
    "        }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def shuffle_data(en_inputs, de_inputs, de_labels, shuffle_inds=None):\n",
    "    \"\"\"Shuffle the data randomly (but all of inputs and labels at ones)\"\"\"\n",
    "\n",
    "    if shuffle_inds is None:\n",
    "        # If shuffle_inds are not passed create a shuffling automatically\n",
    "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        # Shuffle the provided shuffle_inds\n",
    "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
    "\n",
    "    # Return shuffled data\n",
    "    return (\n",
    "        en_inputs[shuffle_inds],\n",
    "        de_inputs[shuffle_inds],\n",
    "        de_labels[shuffle_inds],\n",
    "    ), shuffle_inds\n",
    "\n",
    "\n",
    "def check_for_nans(loss, model, en_lookup_layer, de_lookup_layer):\n",
    "\n",
    "    if np.isnan(loss):\n",
    "        for r_i in range(len(y)):\n",
    "            loss_sample, _ = model.evaluate(\n",
    "                [x[0][r_i : r_i + 1], x[1][r_i : r_i + 1]], y[r_i : r_i + 1], verbose=0\n",
    "            )\n",
    "            if np.isnan(loss_sample):\n",
    "\n",
    "                print(\"=\" * 25, \"nan detected\", \"=\" * 25)\n",
    "                print(\"train_batch\", i, \"r_i\", r_i)\n",
    "                print(\"en_input ->\", x[0][r_i].tolist())\n",
    "                print(\"en_input_wid ->\", en_lookup_layer(x[0][r_i]).numpy().tolist())\n",
    "                print(\"de_input ->\", x[1][r_i].tolist())\n",
    "                print(\"de_input_wid ->\", de_lookup_layer(x[1][r_i]).numpy().tolist())\n",
    "                print(\"de_output_wid ->\", y[r_i].tolist())\n",
    "\n",
    "                if r_i > 0:\n",
    "                    print(\"=\" * 25, \"no-nan\", \"=\" * 25)\n",
    "                    print(\"en_input ->\", x[0][r_i - 1].tolist())\n",
    "                    print(\n",
    "                        \"en_input_wid ->\",\n",
    "                        en_lookup_layer(x[0][r_i - 1]).numpy().tolist(),\n",
    "                    )\n",
    "                    print(\"de_input ->\", x[1][r_i - 1].tolist())\n",
    "                    print(\n",
    "                        \"de_input_wid ->\",\n",
    "                        de_lookup_layer(x[1][r_i - 1]).numpy().tolist(),\n",
    "                    )\n",
    "                    print(\"de_output_wid ->\", y[r_i - 1].tolist())\n",
    "                    return\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    en_lookup_layer,\n",
    "    de_lookup_layer,\n",
    "    train_xy,\n",
    "    valid_xy,\n",
    "    test_xy,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    predict_bleu_at_training=False,\n",
    "):\n",
    "    \"\"\"Training the model and evaluating on validation/test sets\"\"\"\n",
    "\n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(fr_vocabulary)\n",
    "\n",
    "    # Define the data\n",
    "    data_dict = prepare_data(de_lookup_layer, train_xy, valid_xy, test_xy)\n",
    "\n",
    "    shuffle_inds = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Reset metric logs every epoch\n",
    "        if predict_bleu_at_training:\n",
    "            blue_log = []\n",
    "        accuracy_log = []\n",
    "        loss_log = []\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                         Train Phase                                 #\n",
    "        # =================================================================== #\n",
    "\n",
    "        # Shuffle data at the beginning of every epoch\n",
    "        if shuffle:\n",
    "            (en_inputs_raw, de_inputs_raw, de_labels), shuffle_inds = shuffle_data(\n",
    "                data_dict[\"train\"][\"encoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_labels\"],\n",
    "                shuffle_inds,\n",
    "            )\n",
    "        else:\n",
    "            (en_inputs_raw, de_inputs_raw, de_labels) = (\n",
    "                data_dict[\"train\"][\"encoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_labels\"],\n",
    "            )\n",
    "        # Get the number of training batches\n",
    "        n_train_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "        prev_loss = None\n",
    "        # Train one batch at a time\n",
    "        for i in range(n_train_batches):\n",
    "            # Status update\n",
    "            print(f\"Training batch {i+1}/{n_train_batches}\", end=\"\\r\")\n",
    "\n",
    "            # Get a batch of inputs (english and german sequences)\n",
    "            x = [\n",
    "                en_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "                de_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "            ]\n",
    "            # Get a batch of targets (german sequences offset by 1)\n",
    "            y = de_labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "\n",
    "            # Check if any samples are causing NaNs\n",
    "            check_for_nans(loss, model, en_lookup_layer, de_lookup_layer)\n",
    "\n",
    "            # Train for a single step\n",
    "            model.train_on_batch(x, y)\n",
    "            # Evaluate the model to get the metrics\n",
    "            # loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "\n",
    "            # Update the epoch's log records of the metrics\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "\n",
    "            if predict_bleu_at_training:\n",
    "                # Get the final prediction to compute BLEU\n",
    "                pred_y = model.predict(x)\n",
    "                bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "\n",
    "        print(\"\")\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        if predict_bleu_at_training:\n",
    "            print(\n",
    "                f\"\\t(train) loss: {np.mean(loss_log)} - accuracy: {np.mean(accuracy_log)} - bleu: {np.mean(bleu_log)}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"\\t(train) loss: {np.mean(loss_log)} - accuracy: {np.mean(accuracy_log)}\"\n",
    "            )\n",
    "        # =================================================================== #\n",
    "        #                      Validation Phase                               #\n",
    "        # =================================================================== #\n",
    "\n",
    "        val_en_inputs = data_dict[\"valid\"][\"encoder_inputs\"]\n",
    "        val_de_inputs = data_dict[\"valid\"][\"decoder_inputs\"]\n",
    "        val_de_labels = data_dict[\"valid\"][\"decoder_labels\"]\n",
    "\n",
    "        val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "            model,\n",
    "            de_lookup_layer,\n",
    "            val_en_inputs,\n",
    "            val_de_inputs,\n",
    "            val_de_labels,\n",
    "            batch_size,\n",
    "        )\n",
    "\n",
    "        # Print the evaluation metrics of each epoch\n",
    "        print(\n",
    "            f\"\\t(valid) loss: {val_loss} - accuracy: {val_accuracy} - bleu: {val_bleu}\"\n",
    "        )\n",
    "\n",
    "    # =================================================================== #\n",
    "    #                      Test Phase                                     #\n",
    "    # =================================================================== #\n",
    "\n",
    "    test_en_inputs = data_dict[\"test\"][\"encoder_inputs\"]\n",
    "    test_de_inputs = data_dict[\"test\"][\"decoder_inputs\"]\n",
    "    test_de_labels = data_dict[\"test\"][\"decoder_labels\"]\n",
    "\n",
    "    test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "        model,\n",
    "        de_lookup_layer,\n",
    "        test_en_inputs,\n",
    "        test_de_inputs,\n",
    "        test_de_labels,\n",
    "        batch_size,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n(test) loss: {test_loss} - accuracy: {test_accuracy} - bleu: {test_bleu}\")\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model, de_lookup_layer, en_inputs_raw, de_inputs_raw, de_labels, batch_size\n",
    "):\n",
    "    \"\"\"Evaluate the model on various metrics such as loss, accuracy and BLEU\"\"\"\n",
    "\n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "\n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    # Get the number of batches\n",
    "    n_batches = en_inputs_raw.shape[0] // batch_size\n",
    "    print(\" \", end=\"\\r\")\n",
    "\n",
    "    # Evaluate one batch at a time\n",
    "    for i in range(n_batches):\n",
    "        # Status update\n",
    "        print(f\"Evaluating batch {i+1}/{n_batches}\", end=\"\\r\")\n",
    "\n",
    "        # Get the inputs and targers\n",
    "        x = [\n",
    "            en_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "            de_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "        ]\n",
    "        y = de_labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "        # Get the evaluation metrics\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        # Get the predictions to compute BLEU\n",
    "        pred_y = model.predict(x)\n",
    "\n",
    "        # Update logs\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "\n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    en_lookup_layer,\n",
    "    fr_lookup_layer,\n",
    "    train_xy,\n",
    "    valid_xy,\n",
    "    test_xy,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    predict_bleu_at_training=False,\n",
    "):\n",
    "    \"\"\"Training the model and evaluating on validation/test sets\"\"\"\n",
    "\n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(en_vocabulary)\n",
    "\n",
    "    # Define the data\n",
    "    data_dict = prepare_data(fr_lookup_layer, train_xy, valid_xy, test_xy)\n",
    "\n",
    "    shuffle_inds = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Reset metric logs every epoch\n",
    "        if predict_bleu_at_training:\n",
    "            bleu_log = []\n",
    "        accuracy_log = []\n",
    "        loss_log = []\n",
    "\n",
    "        # ========================================================== #\n",
    "        #                   Train Phase                              #\n",
    "        # ========================================================== #\n",
    "\n",
    "        # shuffle data at the beginning of every epoch\n",
    "        if shuffle:\n",
    "            (en_inputs_raw, fr_inputs_raw, fr_labels), shuffle_inds = shuffle_data(\n",
    "                data_dict[\"train\"][\"encoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_labels\"],\n",
    "                shuffle_inds,\n",
    "            )\n",
    "        else:\n",
    "            (en_inputs_raw, de_inputs_raw, de_labels) = (\n",
    "                data_dict[\"train\"][\"encoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_labels\"],\n",
    "            )\n",
    "\n",
    "        # Get the number of training batches\n",
    "        n_train_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "        prev_loss = None\n",
    "        # Train one batch at a time\n",
    "        for i in range(n_train_batches):\n",
    "            # Status update\n",
    "            print(\"Training batch {}/{}\".format(i + 1, n_train_batches), end=\"\\r\")\n",
    "\n",
    "            # Get a batch of inputs (english and french sequences)\n",
    "            x = [\n",
    "                en_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "                fr_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "            ]\n",
    "\n",
    "            # Get a batch of targets (french sequences offset by 1)\n",
    "            y = fr_labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "\n",
    "            # check if any samples are causing NaNs\n",
    "            check_for_nans(loss, model, en_lookup_layer, fr_lookup_layer)\n",
    "\n",
    "            # Train for a single step\n",
    "            model.train_on_batch(x, y)\n",
    "\n",
    "            # Update the epoch's log records of the metrics\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "\n",
    "            if predict_bleu_at_training:\n",
    "                # Get the final prediction to compute BLEU\n",
    "                pred_y = model.predict(x)\n",
    "                bleu_log.append(bleu_metric.calculate_bleu_from_predictions(pred_y))\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"\\nEpoch {}/{}\".format(epoch + 1, epochs))\n",
    "            if predict_bleu_at_training:\n",
    "\n",
    "                print(\n",
    "                    f\"\\t(train) loss: {np.mean(loss_log)} - accuracy:{np.mean(accuracy_log)} - bleu: {np.mean(bleu_log)}\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"\\t(train) loss: {np.mean(loss_log)} - accuracy: {np.mean(accuracy_log)}\"\n",
    "                )\n",
    "\n",
    "            # ========================================================== #\n",
    "            #                   Validation Phase                         #\n",
    "            # ========================================================== #\n",
    "\n",
    "            val_en_inputs = data_dict[\"valid\"][\"encoder_inputs\"]\n",
    "            val_fr_inputs = data_dict[\"valid\"][\"decoder_inputs\"]\n",
    "            val_fr_labels = data_dict[\"valid\"][\"decoder_labels\"]\n",
    "\n",
    "            val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "                model,\n",
    "                fr_lookup_layer,\n",
    "                val_en_inputs,\n",
    "                val_fr_inputs,\n",
    "                val_fr_labels,\n",
    "                batch_size,\n",
    "            )\n",
    "            # Print the evaluation metrics of each epoch\n",
    "            print(\n",
    "                \"\\t(valid) loss: {} - accuracy: {} - bleu: {}\".format(\n",
    "                    val_loss, val_accuracy, val_bleu\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ========================================================== #\n",
    "        #                   test Phase                               #\n",
    "        # ========================================================== #\n",
    "\n",
    "        test_en_inputs = data_dict[\"test\"][\"encoder_inputs\"]\n",
    "        test_fr_inputs = data_dict[\"test\"][\"decoder_inputs\"]\n",
    "        test_fr_labels = data_dict[\"test\"][\"decoder_labels\"]\n",
    "        test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "            model,\n",
    "            fr_lookup_layer,\n",
    "            test_en_inputs,\n",
    "            test_fr_inputs,\n",
    "            test_fr_labels,\n",
    "            batch_size,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"\\n(test) loss: {} - accuracy: {} - bleu: {}\".format(\n",
    "                test_loss, test_accuracy, test_bleu\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualizing attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_matrix_for_sampled_data(\n",
    "    attention_model, target_lookup_layer, test_xy, n_samples=5\n",
    "):\n",
    "    test_x, test_y = test_xy\n",
    "    rand_ids = np.random.randint(0, len(test_xy[0]), size=(n_samples,))\n",
    "    results = []\n",
    "    for rid in rand_ids:\n",
    "        en_input = test_x[rid : rid + 1]\n",
    "        fr_input = test_y[rid : rid + 1, :-1]\n",
    "        attn_weights, predictions = attention_model.predict([en_input, fr_input])\n",
    "        predicted_word_ids = np.argmax(predictions, axis=-1).ravel()\n",
    "        predicted_words = [\n",
    "            target_lookup_layer.get_vocabulary()[wid] for wid in predicted_word_ids\n",
    "        ]\n",
    "        clean_en_input = []\n",
    "        en_start_i = 0\n",
    "        for i, w in enumerate(en_input.ravel()):\n",
    "            if w == \"<pad>\":\n",
    "                en_start_i = i + 1\n",
    "                continue\n",
    "            clean_en_input.append(w)\n",
    "            if w == \"</s>\":\n",
    "                break\n",
    "        clean_predicted_words = []\n",
    "        for w in predicted_words:\n",
    "            clean_predicted_words.append(w)\n",
    "            if w == \"</s>\":\n",
    "                break\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"attention_weights\": attn_weights[\n",
    "                        0,\n",
    "                        : len(clean_predicted_words),\n",
    "                        en_start_i : en_start_i + len(clean_en_input),\n",
    "                    ],\n",
    "                    \"input_words\": clean_en_input,\n",
    "                    \"predicted_words\": clean_predicted_words,\n",
    "                }\n",
    "            )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_sentences_padded_ID = text_vectorizer_en(str(train_en_sentences_padded))\n",
    "train_fr_sentences_padded_ID = text_vectorizer_fr(str(train_fr_sentences_padded))\n",
    "\n",
    "\n",
    "valid_en_sentences_padded_ID = text_vectorizer_en(str(train_en_sentences_padded))\n",
    "valid_fr_sentences_padded_ID = text_vectorizer_fr(str(train_fr_sentences_padded))\n",
    "\n",
    "test_en_sentences_padded_ID = text_vectorizer_en(str(test_en_sentences_padded))\n",
    "test_fr_sentences_padded_ID = text_vectorizer_fr(str(test_fr_sentences_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 12:29:25.713722: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Index out of range using input dim 1; input has only 1 dims\n",
      "2024-10-15 12:29:25.713766: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Index out of range using input dim 1; input has only 1 dims\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m72\u001b[39m\n\u001b[1;32m      6\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq2seq_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43men_lookup_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfr_lookup_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_en_sentences_padded_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_fr_sentences_padded_ID\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_en_sentences_padded_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_fr_sentences_padded_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_en_sentences_padded_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_fr_sentences_padded_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt2\u001b[38;5;241m-\u001b[39mt1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds to complete the training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[82], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, en_lookup_layer, fr_lookup_layer, train_xy, valid_xy, test_xy, epochs, batch_size, shuffle, predict_bleu_at_training)\u001b[0m\n\u001b[1;32m     16\u001b[0m bleu_metric \u001b[38;5;241m=\u001b[39m BLEUMetric(en_vocabulary)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Define the data\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfr_lookup_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_xy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_xy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_xy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m shuffle_inds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Reset metric logs every epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[81], line 14\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(de_lookup_layer, train_xy, valid_xy, test_xy)\u001b[0m\n\u001b[1;32m     12\u001b[0m data_x, data_y \u001b[38;5;241m=\u001b[39m data_xy\n\u001b[1;32m     13\u001b[0m en_inputs \u001b[38;5;241m=\u001b[39m data_x\n\u001b[0;32m---> 14\u001b[0m de_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mdata_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m de_labels \u001b[38;5;241m=\u001b[39m de_lookup_layer(data_y[:, \u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     16\u001b[0m data_dict[label] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: en_inputs,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: de_inputs,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m: de_labels,\n\u001b[1;32m     20\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 72\n",
    "\n",
    "t1 = time.time()\n",
    "train_model(\n",
    "    seq2seq_model,\n",
    "    en_lookup_layer,\n",
    "    fr_lookup_layer,\n",
    "    (train_en_sentences_padded_ID, train_fr_sentences_padded_ID),\n",
    "    (\n",
    "        valid_en_sentences_padded_ID,\n",
    "        valid_fr_sentences_padded_ID,\n",
    "    ),\n",
    "    (\n",
    "        test_en_sentences_padded_ID,\n",
    "        test_fr_sentences_padded_ID,\n",
    "    ),\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"\\nIt took {t2-t1} seconds to complete the training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
