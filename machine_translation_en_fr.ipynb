{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# The data\n",
    "\n",
    "https://www.kaggle.com/datasets/mohamedlotfy50/wmt-2014-english-french/data\n",
    "There over 4.5 million sentence pairs available. However, I will only use 25,000 pairs due to computational feasiblility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Reprise de la session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Je déclare reprise la session du Parlement eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Comme vous avez pu le constater, le grand \"bog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Vous avez souhaité un débat à ce sujet dans le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>En attendant, je souhaiterais, comme un certai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                                  fr  \n",
       "0                              Reprise de la session  \n",
       "1  Je déclare reprise la session du Parlement eur...  \n",
       "2  Comme vous avez pu le constater, le grand \"bog...  \n",
       "3  Vous avez souhaité un débat à ce sujet dans le...  \n",
       "4  En attendant, je souhaiterais, comme un certai...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_sentences = 25000\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"./data/en-fr/wmt14_translate_fr-en_train.csv\", nrows=n_sentences\n",
    ").dropna()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spliting the sentences into tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  On the one hand, we receive all kinds of words of praise for the rapporteur but, at the same time, Mrs Morgan wants to pursue a sort of scorched earth policy on this point and on other key points as well, as a result of which, in fact, the entire content of this report vanishes.\n",
      "French:  D' un côté, nous entendons toutes sortes de louanges au sujet du rapporteur et, simultanément, Mme Morgan veut poursuivre une sorte stratégie de la terre brûlée, à la fois sur ce point précis mais aussi sur un certain nombre d' autres points importants d' ailleurs, qui a pour effet de vider de sa substance la totalité du rapport. \n",
      "\n",
      "English:  Since then, the victims' families, who have not had the consolation of justice, since the pilot responsible has not faced criminal charges, have been awaiting solicitous financial compensation from the United States at the very least.\n",
      "French:  Les familles des victimes, qui n'ont pas eu le sentiment que justice a été faite dès lors que le pilote responsable n'a pas encouru la moindre peine, attendent, au moins, des États-Unis, un dédommagement économique rapide. \n",
      "\n",
      "English:  Actions which promote understanding between young farmers and the EU and also create stronger links with EU candidate countries and the outside world are important.\n",
      "French:  Des actions qui encouragent une meilleure compréhension entre les jeunes agriculteurs et l'UE et créent aussi des liens plus étroits avec les pays candidats à l'UE et le monde extérieur sont importantes. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "original_en_sentences = [sent.strip().split(\" \") for sent in data[\"en\"]]\n",
    "original_fr_sentences = [sent.strip().split(\" \") for sent in data[\"fr\"]]\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(original_en_sentences[index]))\n",
    "    print(\"French: \", \" \".join(original_fr_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding special tokens\n",
    "\n",
    "#### I will add \"< s >\" to mark the start of a sentence and \"< /s >\" to mark the end of a sentence\n",
    "\n",
    "This way\n",
    "we prediction can be done for an arbitrary number of time steps. Using < s > as the starting token gives a\n",
    "way to signal to the decoder that it should start predicting tokens from the target language.\n",
    "\n",
    "if < /s > token is not used to mark the end of a sentence, the decoder cannot be signaled to\n",
    "end a sentence. This can lead the model to enter an infinite loop of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  <s> As your Parliament is aware, the Commission is convinced that, without fundamental reform, the Community judicial bodies run the risk - in the short term - of no longer being able to carry out their task within reasonable deadlines. </s>\n",
      "German:  <s> Comme le sait votre Parlement, la Commission est convaincue que, sans une réforme en profondeur, les instances juridiques communautaires courent le risque de n'être plus à même, à court terme, de remplir leur mission dans des délais raisonnables. </s> \n",
      "\n",
      "English:  <s> Reread the letter: The reference to the fundamental values of the Union is taken - and this is more than mere coincidence - from Article 6 of the Treaty, I repeat, Article 6 of the Treaty. </s>\n",
      "German:  <s> Relisez la lettre et vous verrez que le rappel aux valeurs fondamentales de l'Union est tiré - ce n'est pas un hasard - de l'article 6 du Traité. </s> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_en_sentences]\n",
    "fr_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_fr_sentences]\n",
    "\n",
    "for i in range(2):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(en_sentences[index]))\n",
    "    print(\"German: \", \" \".join(fr_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting training and validation dataset\n",
    "\n",
    "#### 80% training, 10% validation and 10% for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Two', 'major', 'areas', 'of', 'concern', 'about', 'the', 'proposal', 'were', 'considered', 'by', 'the', 'Committee.', '</s>']\n",
      "['<s>', 'Deux', 'domaines', 'ont', 'été', \"l'objet\", 'de', 'préoccupation', 'de', 'la', 'part', 'de', 'la', 'commission', 'en', 'ce', 'qui', 'concerne', 'la', 'proposition.', '</s>']\n",
      "\n",
      "\n",
      "['<s>', 'You', 'have', 'set', 'a', 'very', 'good', 'example', 'of', 'time-keeping', 'today.', '</s>']\n",
      "['<s>', 'Vous', 'avez', 'aussi', 'montré', 'un', 'très', 'bon', 'exemple', 'de', 'respect', 'du', 'temps', 'de', 'parole.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "(\n",
    "    train_en_sentences,\n",
    "    valid_test_en_sentences,\n",
    "    train_fr_sentences,\n",
    "    valid_test_fr_sentences,\n",
    ") = train_test_split(en_sentences, fr_sentences, test_size=0.2)\n",
    "\n",
    "\n",
    "(valid_en_sentences, test_en_sentences, valid_fr_sentences, test_fr_sentences) = (\n",
    "    train_test_split(valid_test_en_sentences, valid_test_fr_sentences, test_size=0.5)\n",
    ")\n",
    "\n",
    "\n",
    "print(train_en_sentences[1])\n",
    "print(train_fr_sentences[1])\n",
    "print(\"\\n\")\n",
    "print(test_en_sentences[0])\n",
    "print(test_fr_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining sequence leghts fot the two languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        27.677500\n",
       "std         15.823126\n",
       "min          3.000000\n",
       "5%           8.000000\n",
       "50%         25.000000\n",
       "95%         58.000000\n",
       "max        144.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting some basic statistics from the data\n",
    "\n",
    "# convert train_en_sentences to a pandas series\n",
    "pd.Series(train_en_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        28.942850\n",
       "std         16.826451\n",
       "min          3.000000\n",
       "5%           9.000000\n",
       "50%         26.000000\n",
       "95%         61.000000\n",
       "max        154.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_fr_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from the train data statistics above, 95% of the english sentences have lengths of 57 while in the french, 95 % of sentences have lengths of 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding the sentences with pad_sequences from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 08:34:48.144377: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-15 08:34:48.265282: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-15 08:34:48.310311: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-15 08:34:48.323205: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-15 08:34:48.403138: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 08:34:49.271281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>' 'Two' 'major' 'areas' 'of' 'concern' 'about' 'the' 'proposal'\n",
      " 'were' 'considered' 'by' 'the' 'Committee.' '</s>' 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "n_en_seq_length = 60\n",
    "n_fr_seq_length = 60\n",
    "unk_token = \"<unk>\"\n",
    "\n",
    "train_en_sentences_padded = pad_sequences(\n",
    "    train_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_en_sentences_padded = pad_sequences(\n",
    "    valid_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_en_sentences_padded = pad_sequences(\n",
    "    test_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "\n",
    "train_fr_sentences_padded = pad_sequences(\n",
    "    train_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_fr_sentences_padded = pad_sequences(\n",
    "    valid_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_fr_sentences_padded = pad_sequences(\n",
    "    test_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "print(train_en_sentences_padded[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728995692.687307    1136 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728995692.776347    1136 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728995692.776396    1136 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728995692.779388    1136 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728995692.779429    1136 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728995692.779447    1136 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728995692.937914    1136 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728995692.937977    1136 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-15 08:34:52.937985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1728995692.938015    1136 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-15 08:34:52.938479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import os\n",
    "\n",
    "# using text vectorization\n",
    "# text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_fr = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_en.adapt(data[\"en\"])\n",
    "# text_vectorizer_fr.adapt(data[\"fr\"])\n",
    "\n",
    "en_vocabulary = []\n",
    "with open(os.path.join(\"./data/en-fr\", \"vocab.en\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        en_vocabulary.append(row.strip())\n",
    "\n",
    "fr_vocabulary = []\n",
    "with open(os.path.join(\"./data/en-fr\", \"vocab.fr\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        fr_vocabulary.append(row.strip())\n",
    "\n",
    "text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_fr = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_en.adapt(en_vocabulary)\n",
    "text_vectorizer_fr.adapt(fr_vocabulary)\n",
    "\n",
    "\n",
    "en_vocabulary = text_vectorizer_en.get_vocabulary()\n",
    "fr_vocabulary = text_vectorizer_fr.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[UNK]', '[UNK]')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_unk_token = en_vocabulary.pop(1)\n",
    "fr_unk_token = fr_vocabulary.pop(1)\n",
    "\n",
    "en_unk_token, fr_unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "pad_token = \"[PAD]\"\n",
    "\n",
    "# English look up layer\n",
    "en_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=en_vocabulary,\n",
    "    oov_token=en_unk_token,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")\n",
    "\n",
    "# French look up layer\n",
    "fr_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=fr_vocabulary,\n",
    "    oov_token=en_unk_token,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(en_lookup_layer)\n",
    "# en_lookup_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes n_en_seq_length of sentences\n",
    "encoder_input = tf.keras.layers.Input(shape=(n_en_seq_length,), dtype=tf.string)\n",
    "\n",
    "# using lookup layer into word IDs\n",
    "encoder_wid_out = en_lookup_layer(encoder_input)\n",
    "\n",
    "\"\"\"\n",
    "With the tokens converted into IDs, route the generated word IDs to a token embedding layer.\n",
    "Pass in the size of the vocabulary (derived from the en_lookup_layer's get_vocabulary()\n",
    "method) and the embedding size (128) and finally then ask the layer to mask any zero-valued inputs\n",
    "as they don’t contain any information:\n",
    "\n",
    "\"\"\"\n",
    "en_full_vocab_size = len(en_lookup_layer.get_vocabulary())\n",
    "encoder_emb_out = tf.keras.layers.Embedding(en_full_vocab_size, 128, mask_zero=True)(\n",
    "    encoder_wid_out\n",
    ")\n",
    "\n",
    "\n",
    "encoder_gru_out, encoder_gru_last_state = tf.keras.layers.GRU(\n",
    "    256, return_sequences=True, return_state=True\n",
    ")(encoder_emb_out)\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Decoder with teacher forcing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = tf.keras.layers.Input(shape=(n_fr_seq_length - 1,), dtype=tf.string)\n",
    "\n",
    "# convert tokens to IDs using the de_lookup_layer\n",
    "decoder_wid_out = fr_lookup_layer(decoder_input)\n",
    "\n",
    "# decoder embedding layer\n",
    "fr_full_vocab_size = len(fr_lookup_layer.get_vocabulary())\n",
    "decoder_emb_out = tf.keras.layers.Embedding(fr_full_vocab_size, 128, mask_zero=True)(\n",
    "    decoder_wid_out\n",
    ")\n",
    "\n",
    "# decoder layer>>> pass the last state of the encoder into the decoder\n",
    "decoder_gru_out = tf.keras.layers.GRU(256, return_sequences=True)(\n",
    "    decoder_emb_out, initial_state=encoder_gru_last_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Badanau Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # Weights to compute Bahdanau attention\n",
    "        self.Wa = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.Ua = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention(\n",
    "            use_scale=True\n",
    "        )  # takes query, key and value\n",
    "        # query = each decoder GRU's hidden states for eact time step.\n",
    "\n",
    "    def call(self, query, key, value, mask, return_attention_scores=False):\n",
    "\n",
    "        # compute 'Wa.ht'.\n",
    "\n",
    "        wa_query = self.Wa(query)\n",
    "\n",
    "        # Compute Ua.hs\n",
    "        ua_key = self.Ua(key)\n",
    "\n",
    "        # compute masks\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        # Compute the attention\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs=[wa_query, value, ua_key],\n",
    "            mask=[query_mask, value_mask, value_mask],\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "\n",
    "        if not return_attention_scores:\n",
    "            return context_vector\n",
    "        else:\n",
    "            return context_vector, attention_weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
