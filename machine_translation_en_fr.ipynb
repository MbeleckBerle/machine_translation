{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# The data\n",
    "\n",
    "https://www.kaggle.com/datasets/mohamedlotfy50/wmt-2014-english-french/data\n",
    "There over 4.5 million sentence pairs available. However, I will only use 25,000 pairs due to computational feasiblility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Reprise de la session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Je déclare reprise la session du Parlement eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Comme vous avez pu le constater, le grand \"bog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Vous avez souhaité un débat à ce sujet dans le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>En attendant, je souhaiterais, comme un certai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                                  fr  \n",
       "0                              Reprise de la session  \n",
       "1  Je déclare reprise la session du Parlement eur...  \n",
       "2  Comme vous avez pu le constater, le grand \"bog...  \n",
       "3  Vous avez souhaité un débat à ce sujet dans le...  \n",
       "4  En attendant, je souhaiterais, comme un certai...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_sentences = 25000\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"./data/en-fr/wmt14_translate_fr-en_train.csv\", nrows=n_sentences\n",
    ").dropna()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spliting the sentences into tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  It has been used by the Community for some time now in a range of policy areas, environmental, human, animal and plant health; and it is explicitly mentioned in the environmental provisions of the EC Treaty since Maastricht.\n",
      "French:  La Communauté l'utilise maintenant depuis un certain temps dans toute une série de politiques relatives à l'environnement et à la santé des êtres humains, des animaux et des plantes. Il en est fait explicitement mention dans les dispositions environnementales du traité CE depuis Maastricht. \n",
      "\n",
      "English:  The Union' s economic and social policy is just as much to blame for that as its regional policy.\n",
      "French:  Tout cela est à imputer tant à la politique régionale qu'à la politique économique et sociale plus générale de l'Union. \n",
      "\n",
      "English:  Think of specialist, national courts with the option of direct appeal to a special competition court at the European Court.\n",
      "French:  Je pense à des tribunaux nationaux spécialisés avec une possibilité de recours direct auprès d' une chambre spéciale de la concurrence rattachée à la Cour de justice européenne. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "original_en_sentences = [sent.strip().split(\" \") for sent in data[\"en\"]]\n",
    "original_fr_sentences = [sent.strip().split(\" \") for sent in data[\"fr\"]]\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(original_en_sentences[index]))\n",
    "    print(\"French: \", \" \".join(original_fr_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding special tokens\n",
    "\n",
    "#### I will add \"< s >\" to mark the start of a sentence and \"< /s >\" to mark the end of a sentence\n",
    "\n",
    "This way\n",
    "we prediction can be done for an arbitrary number of time steps. Using < s > as the starting token gives a\n",
    "way to signal to the decoder that it should start predicting tokens from the target language.\n",
    "\n",
    "if < /s > token is not used to mark the end of a sentence, the decoder cannot be signaled to\n",
    "end a sentence. This can lead the model to enter an infinite loop of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  <s> And our current EU mantra is AFSJ: Area for Freedom, Security and Justice. </s>\n",
      "German:  <s> Le mantra européen d'aujourd'hui s'appelle ELSJ - espace de liberté, de sécurité et de justice. </s> \n",
      "\n",
      "English:  <s> On the other hand, we are witnessing the birth of a political Europe, which entails a collection of shared values which we all agree upon, and a European Union at the service of the citizens. </s>\n",
      "German:  <s> Par ailleurs, nous vivons la naissance de l'Europe politique, qui est une communauté de valeurs sur laquelle nous sommes d'accord, et une Union au service du citoyen. </s> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_en_sentences]\n",
    "fr_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_fr_sentences]\n",
    "\n",
    "for i in range(2):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(en_sentences[index]))\n",
    "    print(\"German: \", \" \".join(fr_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting training and validation dataset\n",
    "\n",
    "#### 80% training, 10% validation and 10% for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'So', 'Parliament', 'should', 'send', 'a', 'message,', 'since', 'that', 'is', 'the', 'wish', 'of', 'the', 'vast', 'majority.', '</s>']\n",
      "['<s>', 'Le', 'Parlement', 'devrait', 'dès', 'lors', 'envoyer', 'un', 'message', 'en', 'ce', 'sens,', 'étant', 'donné', \"qu'une\", 'grande', 'majorité', 'des', 'députés', 'le', 'souhaite.', '</s>']\n",
      "\n",
      "\n",
      "['<s>', 'However,', 'we', 'are', 'not', 'convinced', 'that', 'one', 'form', 'of', 'discrimination', 'should', 'be', 'countered', 'with', 'another', 'form', 'of', 'discrimination,', 'namely', 'the', 'introduction', 'of', 'quotas', 'to', 'guarantee', 'women', 'elected', 'places.', '</s>']\n",
      "['<s>', 'Toutefois,', 'nous', 'ne', 'sommes', 'pas', 'convaincus', \"qu'une\", 'discrimination', 'devrait', 'être', 'contrée', 'par', 'une', 'autre,', 'à', 'savoir', \"l'introduction\", 'de', 'quotas', 'en', 'vue', 'de', 'garantir', 'des', 'sièges', 'pour', 'les', 'femmes.', '</s>']\n",
      "Train size: 20000\n",
      "Valid size: 2500\n",
      "Test size: 2500\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "(\n",
    "    train_en_sentences,\n",
    "    valid_test_en_sentences,\n",
    "    train_fr_sentences,\n",
    "    valid_test_fr_sentences,\n",
    ") = train_test_split(en_sentences, fr_sentences, test_size=0.2)\n",
    "\n",
    "\n",
    "(valid_en_sentences, test_en_sentences, valid_fr_sentences, test_fr_sentences) = (\n",
    "    train_test_split(valid_test_en_sentences, valid_test_fr_sentences, test_size=0.5)\n",
    ")\n",
    "\n",
    "\n",
    "print(train_en_sentences[1])\n",
    "print(train_fr_sentences[1])\n",
    "print(\"\\n\")\n",
    "print(test_en_sentences[0])\n",
    "print(test_fr_sentences[0])\n",
    "\n",
    "print(f\"Train size: {len(train_en_sentences)}\")\n",
    "print(f\"Valid size: {len(valid_en_sentences)}\")\n",
    "print(f\"Test size: {len(test_en_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining sequence leghts fot the two languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        27.610450\n",
       "std         15.850302\n",
       "min          3.000000\n",
       "5%           8.000000\n",
       "50%         24.000000\n",
       "95%         58.000000\n",
       "max        150.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting some basic statistics from the data\n",
    "\n",
    "# convert train_en_sentences to a pandas series\n",
    "pd.Series(train_en_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        28.907100\n",
       "std         16.854162\n",
       "min          3.000000\n",
       "5%           8.000000\n",
       "50%         26.000000\n",
       "95%         61.000000\n",
       "max        154.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_fr_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from the train data statistics above, 95% of the english sentences have lengths of 57 while in the french, 95 % of sentences have lengths of 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding the sentences with pad_sequences from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 12:54:26.775253: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-15 12:54:26.786452: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:476] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1729011266.796811   57878 cuda_dnn.cc:8508] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1729011266.799939   57878 cuda_blas.cc:1420] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-15 12:54:26.812877: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>' 'So' 'Parliament' 'should' 'send' 'a' 'message,' 'since' 'that'\n",
      " 'is' 'the' 'wish' 'of' 'the' 'vast' 'majority.' '</s>' '[pad]' '[pad]'\n",
      " '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]'\n",
      " '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]'\n",
      " '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]'\n",
      " '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]' '[pad]'\n",
      " '[pad]' '[pad]' '[pad]' '[pad]' '[pad]']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "n_en_seq_length = 60\n",
    "n_fr_seq_length = 60\n",
    "# unk_token = \"<unk>\"\n",
    "pad_token = \"[pad]\"\n",
    "\n",
    "train_en_sentences_padded = pad_sequences(\n",
    "    train_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_en_sentences_padded = pad_sequences(\n",
    "    valid_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_en_sentences_padded = pad_sequences(\n",
    "    test_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "\n",
    "train_fr_sentences_padded = pad_sequences(\n",
    "    train_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_fr_sentences_padded = pad_sequences(\n",
    "    valid_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    value=pad_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_fr_sentences_padded = pad_sequences(\n",
    "    test_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "print(train_en_sentences_padded[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1729011268.758414   57878 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import os\n",
    "\n",
    "# using text vectorization\n",
    "# text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_fr = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_en.adapt(data[\"en\"])\n",
    "# text_vectorizer_fr.adapt(data[\"fr\"])\n",
    "\n",
    "en_vocabulary = []\n",
    "with open(os.path.join(\"./data/en-fr\", \"vocab.en\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        en_vocabulary.append(row.strip())\n",
    "\n",
    "fr_vocabulary = []\n",
    "with open(os.path.join(\"./data/en-fr\", \"vocab.fr\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        fr_vocabulary.append(row.strip())\n",
    "\n",
    "text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_fr = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_en.adapt(en_vocabulary)\n",
    "text_vectorizer_fr.adapt(fr_vocabulary)\n",
    "\n",
    "\n",
    "en_vocabulary = text_vectorizer_en.get_vocabulary()\n",
    "fr_vocabulary = text_vectorizer_fr.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[UNK]', '[UNK]')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_unk_token = en_vocabulary.pop(1)\n",
    "fr_unk_token = fr_vocabulary.pop(1)\n",
    "\n",
    "en_unk_token, fr_unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# pad_token = \"<PAD>\"\n",
    "\n",
    "# English look up layer\n",
    "en_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=en_vocabulary,\n",
    "    oov_token=en_unk_token,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")\n",
    "\n",
    "# French look up layer\n",
    "fr_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=fr_vocabulary,\n",
    "    oov_token=en_unk_token,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word IDs: [269792 395373 156726     75 275279 448899     75 287062      1 454356\n",
      "  94376 397282 159214      1]\n",
      "Sample vocabulary: ['[pad]', '[UNK]', '', 'o', 'av', 're', 'ms', 'm', 'i', 'd']\n"
     ]
    }
   ],
   "source": [
    "wid_sample = en_lookup_layer(\n",
    "    \"iron cement protects the ingot against the hot , abrasive steel casting process .\".split(\n",
    "        \" \"\n",
    "    )\n",
    ")\n",
    "print(f\"Word IDs: {wid_sample}\")\n",
    "print(f\"Sample vocabulary: {en_lookup_layer.get_vocabulary()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(en_lookup_layer)\n",
    "# en_lookup_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes n_en_seq_length of sentences\n",
    "encoder_input = tf.keras.layers.Input(shape=(n_en_seq_length,), dtype=tf.string)\n",
    "\n",
    "# using lookup layer into word IDs\n",
    "encoder_wid_out = en_lookup_layer(encoder_input)\n",
    "\n",
    "\"\"\"\n",
    "With the tokens converted into IDs, route the generated word IDs to a token embedding layer.\n",
    "Pass in the size of the vocabulary (derived from the en_lookup_layer's get_vocabulary()\n",
    "method) and the embedding size (128) and finally then ask the layer to mask any zero-valued inputs\n",
    "as they don’t contain any information:\n",
    "\n",
    "\"\"\"\n",
    "en_full_vocab_size = len(en_lookup_layer.get_vocabulary())\n",
    "encoder_emb_out = tf.keras.layers.Embedding(en_full_vocab_size, 128, mask_zero=True)(\n",
    "    encoder_wid_out\n",
    ")\n",
    "\n",
    "\n",
    "encoder_gru_out, encoder_gru_last_state = tf.keras.layers.GRU(\n",
    "    256, return_sequences=True, return_state=True\n",
    ")(encoder_emb_out)\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Decoder with teacher forcing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = tf.keras.layers.Input(shape=(n_fr_seq_length - 1,), dtype=tf.string)\n",
    "\n",
    "# convert tokens to IDs using the de_lookup_layer\n",
    "decoder_wid_out = fr_lookup_layer(decoder_input)\n",
    "\n",
    "# decoder embedding layer\n",
    "fr_full_vocab_size = len(fr_lookup_layer.get_vocabulary())\n",
    "decoder_emb_out = tf.keras.layers.Embedding(fr_full_vocab_size, 128, mask_zero=True)(\n",
    "    decoder_wid_out\n",
    ")\n",
    "\n",
    "# decoder layer>>> pass the last state of the encoder into the decoder\n",
    "decoder_gru_out = tf.keras.layers.GRU(256, return_sequences=True)(\n",
    "    decoder_emb_out, initial_state=encoder_gru_last_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Badanau Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # Weights to compute Bahdanau attention\n",
    "        self.Wa = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.Ua = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention(use_scale=True)\n",
    "\n",
    "    def call(self, query, key, value, mask, return_attention_scores=False):\n",
    "\n",
    "        # Compute `Wa.ht`.\n",
    "        wa_query = self.Wa(query)\n",
    "\n",
    "        # Compute `Ua.hs`.\n",
    "        ua_key = self.Ua(key)\n",
    "\n",
    "        # Compute masks\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        # Compute the attention\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs=[wa_query, value, ua_key],\n",
    "            mask=[query_mask, value_mask, value_mask],\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "\n",
    "        if not return_attention_scores:\n",
    "            return context_vector\n",
    "        else:\n",
    "            return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.keras.backend as K\n",
    "\n",
    "# K.clear_session()\n",
    "\n",
    "# # Defining the encoder layers\n",
    "# encoder_input = tf.keras.layers.Input(shape=(n_en_seq_length,), dtype=tf.string)\n",
    "# # Converting tokens to IDs\n",
    "# encoder_wid_out = en_lookup_layer(encoder_input)\n",
    "\n",
    "# # Embedding layer and lookup\n",
    "# encoder_emb_out = tf.keras.layers.Embedding(\n",
    "#     len(en_lookup_layer.get_vocabulary()), 128, mask_zero=True\n",
    "# )(encoder_wid_out)\n",
    "\n",
    "# # Encoder GRU layer\n",
    "# encoder_gru_out, encoder_gru_last_state = tf.keras.layers.GRU(\n",
    "#     256, return_sequences=True, return_state=True\n",
    "# )(encoder_emb_out)\n",
    "\n",
    "# # Defining the encoder model: in - encoder_input / out - output of the GRU layer\n",
    "# encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)\n",
    "\n",
    "# # Defining the decoder layers\n",
    "# decoder_input = tf.keras.layers.Input(shape=(n_fr_seq_length - 1,), dtype=tf.string)\n",
    "# # Converting tokens to IDs (Decoder)\n",
    "# decoder_wid_out = fr_lookup_layer(decoder_input)\n",
    "\n",
    "# # Embedding layer and lookup (decoder)\n",
    "# full_de_vocab_size = len(fr_lookup_layer.get_vocabulary())\n",
    "# decoder_emb_out = tf.keras.layers.Embedding(full_de_vocab_size, 128, mask_zero=True)(\n",
    "#     decoder_wid_out\n",
    "# )\n",
    "# decoder_gru_out = tf.keras.layers.GRU(256, return_sequences=True)(\n",
    "#     decoder_emb_out, initial_state=encoder_gru_last_state\n",
    "# )\n",
    "\n",
    "# # The attention mechanism (inputs: [q, v, k])\n",
    "# decoder_attn_out, attn_weights = BahdanauAttention(256)(\n",
    "#     query=decoder_gru_out,\n",
    "#     key=encoder_gru_out,\n",
    "#     value=encoder_gru_out,\n",
    "#     mask=(encoder_wid_out != 0),\n",
    "#     return_attention_scores=True,\n",
    "# )\n",
    "\n",
    "# # Concatenate GRU output and the attention output\n",
    "# context_and_rnn_output = tf.keras.layers.Concatenate(axis=-1)(\n",
    "#     [decoder_attn_out, decoder_gru_out]\n",
    "# )\n",
    "\n",
    "# # Final prediction layer (size of the vocabulary)\n",
    "# decoder_out = tf.keras.layers.Dense(full_de_vocab_size, activation=\"softmax\")(\n",
    "#     context_and_rnn_output\n",
    "# )\n",
    "\n",
    "# # Final seq2seq model\n",
    "# seq2seq_model = tf.keras.models.Model(\n",
    "#     inputs=[encoder.inputs, decoder_input], outputs=decoder_out\n",
    "# )\n",
    "\n",
    "# # We will use this model later to visualize attention patterns\n",
    "# attention_visualizer = tf.keras.models.Model(\n",
    "#     inputs=[encoder.inputs, decoder_input], outputs=[attn_weights, decoder_out]\n",
    "# )\n",
    "\n",
    "# # Compiling the model with a loss and an optimizer\n",
    "# seq2seq_model.compile(\n",
    "#     loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "# # Print model summary\n",
    "# seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'bahdanau_attention' (of type BahdanauAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "decoder_attn_out, attn_weights = BahdanauAttention(256)(\n",
    "    query=decoder_gru_out,\n",
    "    key=encoder_gru_out,\n",
    "    value=encoder_gru_out,\n",
    "    mask=(encoder_wid_out != 0),  # mask that denotes which tokens need to be ignored\n",
    "    return_attention_scores=True,\n",
    ")\n",
    "\n",
    "\n",
    "# combine the attention output and the decoder's GRU output to create a\n",
    "# single concatenated input for the prediction\n",
    "context_and_gru_output = tf.keras.layers.Concatenate(axis=-1)(\n",
    "    [decoder_attn_out, decoder_gru_out]\n",
    ")\n",
    "\n",
    "# Prediction layer takes the concatenated attention's context vewctore andthe GRU ouput to\n",
    "# produce probability distributions over the French tokens for each time step\n",
    "decoder_out = tf.keras.layers.Dense(fr_full_vocab_size, activation=\"softmax\")(\n",
    "    context_and_gru_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">StringLookup</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">58,283,136</span> │ string_lookup_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ string_lookup_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">StringLookup</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)           │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,792,832</span> │ string_lookup_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ string_lookup_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">296,448</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bahdanau_attention  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BahdanauAttention</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>)]   │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│                     │                   │            │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bahdanau_attenti… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">59</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21819</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">11,193,147</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mStringLookup\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │ \u001b[38;5;34m58,283,136\u001b[0m │ string_lookup_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ string_lookup_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ string_lookup_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mStringLookup\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)           │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m296,448\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m2,792,832\u001b[0m │ string_lookup_3[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ string_lookup_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m296,448\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bahdanau_attention  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m256\u001b[0m), │    \u001b[38;5;34m131,328\u001b[0m │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mBahdanauAttention\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m60\u001b[0m)]   │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│                     │                   │            │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m512\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ bahdanau_attenti… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m59\u001b[0m, \u001b[38;5;34m21819\u001b[0m) │ \u001b[38;5;34m11,193,147\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72,993,339</span> (278.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m72,993,339\u001b[0m (278.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72,993,339</span> (278.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m72,993,339\u001b[0m (278.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# final end-to-end model\n",
    "seq2seq_model = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=decoder_out\n",
    ")\n",
    "\n",
    "seq2seq_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "seq2seq_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_visualizer = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=[attn_weights, decoder_out]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(fr_lookup_layer, train_xy, valid_xy, test_xy):\n",
    "    \"\"\"\n",
    "    The prepare_data() function takes the source sentence and target sentence pairs and generates\n",
    "    encoder and decoder inputs and decoder labels.\n",
    "\n",
    "    • fr_lookup_layer   =>  The StringLookup layer of the French language\n",
    "    • train_xy          =>  A tuple containing tokenized English sentences and tokenized\n",
    "                            French sentences in the training set, respectively\n",
    "    • valid_xy          =>  Similar to train_xy but for validation data\n",
    "    • test_xy           =>  Similar to train_xy but for test data\n",
    "\n",
    "\n",
    "    For each training, validation, and test dataset, this function generates:\n",
    "\n",
    "    • encoder_inputs => Tokenized English sentences as in the preprocessed dataset\n",
    "    • decoder_inputs => All tokens except the last of each French sentence\n",
    "    • decoder_labels => All token IDs except the first of each French sentence, where token\n",
    "                        IDs are generated by the fr_lookup_layer\n",
    "\n",
    "    decoder_labels will be decoder_inputs shifted one token to the left\n",
    "    \"\"\"\n",
    "    # Create a data dictionary from the dataframes containing data\n",
    "    data_dict = {}\n",
    "    for label, data_xy in zip(\n",
    "        [\"train\", \"valid\", \"test\"], [train_xy, valid_xy, test_xy]\n",
    "    ):\n",
    "        data_x, data_y = data_xy\n",
    "        en_inputs = data_x\n",
    "        fr_inputs = data_y[:, :-1]\n",
    "        fr_labels = fr_lookup_layer(data_y[:, 1:]).numpy()\n",
    "        data_dict[label] = {\n",
    "            \"encoder_input\": en_inputs,\n",
    "            \"decoder_inputs\": fr_inputs,\n",
    "            \"decoder_labels\": fr_labels,\n",
    "        }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(en_inputs, fr_inputs, fr_labels, shuffle_inds=None):\n",
    "    \"\"\"\n",
    "    Shuffle the data randomly (but all inputs and labels at once)\n",
    "    \"\"\"\n",
    "\n",
    "    if shuffle_inds is None:\n",
    "        # If shuffle_inds are not passed, create a shuffle automatically\n",
    "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        # Shuffle the provided shuffle_inds\n",
    "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
    "\n",
    "    # return shuffled data\n",
    "    return (\n",
    "        en_inputs[shuffle_inds],\n",
    "        fr_inputs[shuffle_inds],\n",
    "        fr_labels[shuffle_inds],\n",
    "    ), shuffle_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:32: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:32: SyntaxWarning: invalid escape sequence '\\/'\n",
      "/tmp/ipykernel_57878/1856795582.py:32: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  \"<\\/s>.*\",\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "from bleu import compute_bleu\n",
    "\n",
    "\n",
    "class BLEUMetric(object):\n",
    "\n",
    "    def __init__(self, vocabulary, name=\"perplexity\", **kwargs):\n",
    "        \"\"\"Computes the BLEU score (Metric for machine translation)\"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab = vocabulary\n",
    "        self.id_to_token_layer = StringLookup(vocabulary=self.vocab, invert=True)\n",
    "\n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\"Calculate the BLEU score for targets and predictions\"\"\"\n",
    "\n",
    "        # Get the predicted token IDs\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)\n",
    "\n",
    "        # Convert token IDs to words using the vocabulary and the StringLookup\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
    "        real_tokens = self.id_to_token_layer(real)\n",
    "\n",
    "        def clean_text(tokens):\n",
    "            \"\"\"Clean padding and <s>/</s> tokens to only keep meaningful words\"\"\"\n",
    "\n",
    "            # 3. Strip the string of any extra white spaces\n",
    "            translations_in_bytes = tf.strings.strip(\n",
    "                # 2. Replace everything after the eos token with blank\n",
    "                tf.strings.regex_replace(\n",
    "                    # 1. Join all the tokens to one string in each sequence\n",
    "                    tf.strings.join(tf.transpose(tokens), separator=\" \"),\n",
    "                    \"<\\/s>.*\",\n",
    "                    \"\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Decode the byte stream to a string\n",
    "            translations = np.char.decode(\n",
    "                translations_in_bytes.numpy().astype(np.bytes_), encoding=\"utf-8\"\n",
    "            )\n",
    "\n",
    "            # If the string is empty, add a [UNK] token\n",
    "            # Otherwise get a Division by zero error\n",
    "            translations = [\n",
    "                sent if len(sent) > 0 else en_unk_token for sent in translations\n",
    "            ]\n",
    "\n",
    "            # Split the sequences to individual tokens\n",
    "            translations = np.char.split(translations).tolist()\n",
    "\n",
    "            return translations\n",
    "\n",
    "        # Get the clean versions of the predictions and real seuqences\n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n",
    "        real_tokens = [[token_seq] for token_seq in clean_text(real_tokens)]\n",
    "\n",
    "        # The compute_bleu method accpets the translations and references in the following format\n",
    "        # tranlation - list of list of tokens\n",
    "        # references - list of list of list of tokens\n",
    "        bleu, precisions, bp, ratio, translation_length, reference_length = (\n",
    "            compute_bleu(real_tokens, pred_tokens, smooth=False)\n",
    "        )\n",
    "\n",
    "        return bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    en_lookup_layer,\n",
    "    fr_lookup_layer,\n",
    "    train_xy,\n",
    "    valid_xy,\n",
    "    test_xy,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    predict_bleu_at_training=False,\n",
    "):\n",
    "    \"\"\"Training the model and evaluating on validation/test sets\"\"\"\n",
    "\n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(en_vocabulary)\n",
    "\n",
    "    # Define the data\n",
    "    data_dict = prepare_data(fr_lookup_layer, train_xy, valid_xy, test_xy)\n",
    "\n",
    "    shuffle_inds = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Reset metric logs every epoch\n",
    "        if predict_bleu_at_training:\n",
    "            bleu_log = []\n",
    "        accuracy_log = []\n",
    "        loss_log = []\n",
    "\n",
    "        # ========================================================== #\n",
    "        #                   Train Phase                              #\n",
    "        # ========================================================== #\n",
    "\n",
    "        # shuffle data at the beginning of every epoch\n",
    "        if shuffle:\n",
    "            (en_inputs_raw, fr_inputs_raw, fr_labels), shuffle_inds = shuffle_data(\n",
    "                data_dict[\"train\"][\"encoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_labels\"],\n",
    "                shuffle_inds,\n",
    "            )\n",
    "        else:\n",
    "            (en_inputs_raw, de_inputs_raw, de_labels) = (\n",
    "                data_dict[\"train\"][\"encoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_labels\"],\n",
    "            )\n",
    "\n",
    "        # Get the number of training batches\n",
    "        n_train_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "        prev_loss = None\n",
    "        # Train one batch at a time\n",
    "        for i in range(n_train_batches):\n",
    "            # Status update\n",
    "            print(\"Training batch {}/{}\".format(i + 1, n_train_batches), end=\"\\r\")\n",
    "\n",
    "            # Get a batch of inputs (english and french sequences)\n",
    "            x = [\n",
    "                en_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "                fr_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "            ]\n",
    "\n",
    "            # Get a batch of targets (french sequences offset by 1)\n",
    "            y = fr_labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "\n",
    "            # check if any samples are causing NaNs\n",
    "            check_for_nans(loss, model, en_lookup_layer, fr_lookup_layer)\n",
    "\n",
    "            # Train for a single step\n",
    "            model.train_on_batch(x, y)\n",
    "\n",
    "            # Update the epoch's log records of the metrics\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "\n",
    "            if predict_bleu_at_training:\n",
    "                # Get the final prediction to compute BLEU\n",
    "                pred_y = model.predict(x)\n",
    "                bleu_log.append(bleu_metric.calculate_bleu_from_predictions(pred_y))\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"\\nEpoch {}/{}\".format(epoch + 1, epochs))\n",
    "            if predict_bleu_at_training:\n",
    "\n",
    "                print(\n",
    "                    f\"\\t(train) loss: {np.mean(loss_log)} - accuracy:{np.mean(accuracy_log)} - bleu: {np.mean(bleu_log)}\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"\\t(train) loss: {np.mean(loss_log)} - accuracy: {np.mean(accuracy_log)}\"\n",
    "                )\n",
    "\n",
    "            # ========================================================== #\n",
    "            #                   Validation Phase                         #\n",
    "            # ========================================================== #\n",
    "\n",
    "            val_en_inputs = data_dict[\"valid\"][\"encoder_inputs\"]\n",
    "            val_fr_inputs = data_dict[\"valid\"][\"decoder_inputs\"]\n",
    "            val_fr_labels = data_dict[\"valid\"][\"decoder_labels\"]\n",
    "\n",
    "            val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "                model,\n",
    "                fr_lookup_layer,\n",
    "                val_en_inputs,\n",
    "                val_fr_inputs,\n",
    "                val_fr_labels,\n",
    "                batch_size,\n",
    "            )\n",
    "            # Print the evaluation metrics of each epoch\n",
    "            print(\n",
    "                \"\\t(valid) loss: {} - accuracy: {} - bleu: {}\".format(\n",
    "                    val_loss, val_accuracy, val_bleu\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ========================================================== #\n",
    "        #                   test Phase                               #\n",
    "        # ========================================================== #\n",
    "\n",
    "        test_en_inputs = data_dict[\"test\"][\"encoder_inputs\"]\n",
    "        test_fr_inputs = data_dict[\"test\"][\"decoder_inputs\"]\n",
    "        test_fr_labels = data_dict[\"test\"][\"decoder_labels\"]\n",
    "        test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "            model,\n",
    "            fr_lookup_layer,\n",
    "            test_en_inputs,\n",
    "            test_fr_inputs,\n",
    "            test_fr_labels,\n",
    "            batch_size,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"\\n(test) loss: {} - accuracy: {} - bleu: {}\".format(\n",
    "                test_loss, test_accuracy, test_bleu\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualizing attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_matrix_for_sampled_data(\n",
    "    attention_model, target_lookup_layer, test_xy, n_samples=5\n",
    "):\n",
    "    test_x, test_y = test_xy\n",
    "    rand_ids = np.random.randint(0, len(test_xy[0]), size=(n_samples,))\n",
    "    results = []\n",
    "    for rid in rand_ids:\n",
    "        en_input = test_x[rid : rid + 1]\n",
    "        fr_input = test_y[rid : rid + 1, :-1]\n",
    "        attn_weights, predictions = attention_model.predict([en_input, fr_input])\n",
    "        predicted_word_ids = np.argmax(predictions, axis=-1).ravel()\n",
    "        predicted_words = [\n",
    "            target_lookup_layer.get_vocabulary()[wid] for wid in predicted_word_ids\n",
    "        ]\n",
    "        clean_en_input = []\n",
    "        en_start_i = 0\n",
    "        for i, w in enumerate(en_input.ravel()):\n",
    "            if w == \"<pad>\":\n",
    "                en_start_i = i + 1\n",
    "                continue\n",
    "            clean_en_input.append(w)\n",
    "            if w == \"</s>\":\n",
    "                break\n",
    "        clean_predicted_words = []\n",
    "        for w in predicted_words:\n",
    "            clean_predicted_words.append(w)\n",
    "            if w == \"</s>\":\n",
    "                break\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"attention_weights\": attn_weights[\n",
    "                        0,\n",
    "                        : len(clean_predicted_words),\n",
    "                        en_start_i : en_start_i + len(clean_en_input),\n",
    "                    ],\n",
    "                    \"input_words\": clean_en_input,\n",
    "                    \"predicted_words\": clean_predicted_words,\n",
    "                }\n",
    "            )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en_sentences_padded_ID = text_vectorizer_en(str(train_en_sentences_padded))\n",
    "train_fr_sentences_padded_ID = text_vectorizer_fr(str(train_fr_sentences_padded))\n",
    "\n",
    "\n",
    "valid_en_sentences_padded_ID = text_vectorizer_en(str(train_en_sentences_padded))\n",
    "valid_fr_sentences_padded_ID = text_vectorizer_fr(str(train_fr_sentences_padded))\n",
    "\n",
    "test_en_sentences_padded_ID = text_vectorizer_en(str(test_en_sentences_padded))\n",
    "test_fr_sentences_padded_ID = text_vectorizer_fr(str(test_fr_sentences_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 12:54:32.710848: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: Index out of range using input dim 1; input has only 1 dims\n",
      "2024-10-15 12:54:32.710898: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: Index out of range using input dim 1; input has only 1 dims\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m72\u001b[39m\n\u001b[1;32m      6\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq2seq_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43men_lookup_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfr_lookup_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_en_sentences_padded_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_fr_sentences_padded_ID\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_en_sentences_padded_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_fr_sentences_padded_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_en_sentences_padded_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_fr_sentences_padded_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m t2 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIt took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt2\u001b[38;5;241m-\u001b[39mt1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds to complete the training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, en_lookup_layer, fr_lookup_layer, train_xy, valid_xy, test_xy, epochs, batch_size, shuffle, predict_bleu_at_training)\u001b[0m\n\u001b[1;32m     16\u001b[0m bleu_metric \u001b[38;5;241m=\u001b[39m BLEUMetric(en_vocabulary)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Define the data\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfr_lookup_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_xy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_xy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_xy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m shuffle_inds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Reset metric logs every epoch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 29\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(fr_lookup_layer, train_xy, valid_xy, test_xy)\u001b[0m\n\u001b[1;32m     27\u001b[0m data_x, data_y \u001b[38;5;241m=\u001b[39m data_xy\n\u001b[1;32m     28\u001b[0m en_inputs \u001b[38;5;241m=\u001b[39m data_x\n\u001b[0;32m---> 29\u001b[0m fr_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mdata_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     30\u001b[0m fr_labels \u001b[38;5;241m=\u001b[39m fr_lookup_layer(data_y[:, \u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     31\u001b[0m data_dict[label] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: en_inputs,\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_inputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: fr_inputs,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m: fr_labels,\n\u001b[1;32m     35\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Index out of range using input dim 1; input has only 1 dims [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 72\n",
    "\n",
    "t1 = time.time()\n",
    "train_model(\n",
    "    seq2seq_model,\n",
    "    en_lookup_layer,\n",
    "    fr_lookup_layer,\n",
    "    (train_en_sentences_padded_ID, train_fr_sentences_padded_ID),\n",
    "    (\n",
    "        valid_en_sentences_padded_ID,\n",
    "        valid_fr_sentences_padded_ID,\n",
    "    ),\n",
    "    (\n",
    "        test_en_sentences_padded_ID,\n",
    "        test_fr_sentences_padded_ID,\n",
    "    ),\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "t2 = time.time()\n",
    "\n",
    "print(f\"\\nIt took {t2-t1} seconds to complete the training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
