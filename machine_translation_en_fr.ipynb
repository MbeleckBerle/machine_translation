{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# The data\n",
    "\n",
    "https://www.kaggle.com/datasets/mohamedlotfy50/wmt-2014-english-french/data\n",
    "There over 4.5 million sentence pairs available. However, I will only use 25,000 pairs due to computational feasiblility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Reprise de la session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Je déclare reprise la session du Parlement eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Comme vous avez pu le constater, le grand \"bog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Vous avez souhaité un débat à ce sujet dans le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>En attendant, je souhaiterais, comme un certai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                                  fr  \n",
       "0                              Reprise de la session  \n",
       "1  Je déclare reprise la session du Parlement eur...  \n",
       "2  Comme vous avez pu le constater, le grand \"bog...  \n",
       "3  Vous avez souhaité un débat à ce sujet dans le...  \n",
       "4  En attendant, je souhaiterais, comme un certai...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_sentences = 25000\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"./data/en-fr/wmt14_translate_fr-en_train.csv\", nrows=n_sentences\n",
    ").dropna()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spliting the sentences into tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  I completely understand your concerns, and I feel that there is a need for effort and a certain degree of consistency within the European Union itself on this issue.\n",
      "French:  Je comprends parfaitement les inquiétudes de M. le député et je vois bien que l'on a besoin d'un effort et d'une certaine cohérence y compris à l'intérieur de l'Union européenne en ce qui concerne cette matière. \n",
      "\n",
      "English:  If the world' s other important currency trading areas were to be excluded from the Tobin tax, the fact that tax was liable to be paid in Europe would lead to currency dealing moving to those areas.\n",
      "French:  Si les autres zones de change importantes du monde restaient en dehors de la taxe Tobin, la taxe perçue en Europe aurait pour effet de transférer le marché des changes vers ces zones-là. \n",
      "\n",
      "English:  Three amendments can be partially approved, and one can be approved in principle.\n",
      "French:  Trois amendements peuvent être partiellement adoptés et un peut l'être en principe. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "original_en_sentences = [sent.strip().split(\" \") for sent in data[\"en\"]]\n",
    "original_fr_sentences = [sent.strip().split(\" \") for sent in data[\"fr\"]]\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(original_en_sentences[index]))\n",
    "    print(\"French: \", \" \".join(original_fr_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding special tokens\n",
    "\n",
    "#### I will add \"< s >\" to mark the start of a sentence and \"< /s >\" to mark the end of a sentence\n",
    "\n",
    "This way\n",
    "we prediction can be done for an arbitrary number of time steps. Using < s > as the starting token gives a\n",
    "way to signal to the decoder that it should start predicting tokens from the target language.\n",
    "\n",
    "if < /s > token is not used to mark the end of a sentence, the decoder cannot be signaled to\n",
    "end a sentence. This can lead the model to enter an infinite loop of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  <s> Not only do these decisions conflict with the Treaty and apportion to the institutions of the Union more power than that to which they are entitled but, worst of all, their effect will be counter-productive. </s>\n",
      "German:  <s> Les adoptions de ce jour sont non seulement contraires au Traité et octroient aux organes de l'Union davantage de pouvoirs que ceux qui lui reviennent, mais surtout, elles produiront un effet contraire. </s> \n",
      "\n",
      "English:  <s> I also believe that the Commission, or President Prodi, has acted as a guardian of the Treaties. </s>\n",
      "German:  <s> En outre, je pense que la Commission, Monsieur le Président Prodi, a été gardienne des Traités. </s> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_en_sentences]\n",
    "fr_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_fr_sentences]\n",
    "\n",
    "for i in range(2):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(en_sentences[index]))\n",
    "    print(\"German: \", \" \".join(fr_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting training and validation dataset\n",
    "\n",
    "#### 80% training, 10% validation and 10% for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'The', 'situation', 'should', 'also', 'remind', 'us', 'all', 'that', 'the', 'ratification', 'of', 'the', 'Treaty', 'for', 'an', 'International', 'Criminal', 'Court', 'has', 'been', 'shamefully', 'slow.', '</s>']\n",
      "['<s>', 'La', 'situation', 'devrait', 'nous', 'rappeler', 'aussi', 'le', 'fait', 'que', 'la', 'ratification', 'du', 'traité', 'instituant', 'le', 'Tribunal', 'pénal', 'international', 'a', 'été', 'honteusement', 'lente.', '</s>']\n",
      "\n",
      "\n",
      "['<s>', 'I', 'must', 'say', 'that', 'Alex', \"Langer's\", 'presence', 'here', 'in', 'the', 'European', 'Parliament', 'is', 'sadly', 'missed.', 'His', 'efforts', 'to', 'build', 'peace', 'should', 'serve', 'as', 'an', 'example,', 'to', 'all', 'of', 'us,', 'on', 'how', 'peaceful', 'coexistence', 'can', 'be', 'achieved.', '</s>']\n",
      "['<s>', 'Je', 'tiens', 'à', 'dire', 'que', 'je', 'ressens', 'fortement', \"l'absence\", 'en', 'ce', 'moment,', 'dans', 'ce', 'Parlement,', \"d'Alex\", 'Langer,', 'qui', 'a', 'été', 'un', 'architecte', 'de', 'la', 'paix', 'et', 'qui', 'nous', 'a', 'montré', 'à', 'tous', 'comment', 'on', 'instaure', 'la', 'cohabitation', 'entre', 'les', 'peuples.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "(\n",
    "    train_en_sentences,\n",
    "    valid_test_en_sentences,\n",
    "    train_fr_sentences,\n",
    "    valid_test_fr_sentences,\n",
    ") = train_test_split(en_sentences, fr_sentences, test_size=0.2)\n",
    "\n",
    "\n",
    "(valid_en_sentences, test_en_sentences, valid_fr_sentences, test_fr_sentences) = (\n",
    "    train_test_split(valid_test_en_sentences, valid_test_fr_sentences, test_size=0.5)\n",
    ")\n",
    "\n",
    "\n",
    "print(train_en_sentences[1])\n",
    "print(train_fr_sentences[1])\n",
    "print(\"\\n\")\n",
    "print(test_en_sentences[0])\n",
    "print(test_fr_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining sequence leghts fot the two languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        27.699450\n",
       "std         15.769621\n",
       "min          3.000000\n",
       "5%           8.000000\n",
       "50%         25.000000\n",
       "95%         58.000000\n",
       "max        150.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting some basic statistics from the data\n",
    "\n",
    "# convert train_en_sentences to a pandas series\n",
    "pd.Series(train_en_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        28.995000\n",
       "std         16.805437\n",
       "min          3.000000\n",
       "5%           9.000000\n",
       "50%         26.000000\n",
       "95%         60.000000\n",
       "max        154.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_fr_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from the train data statistics above, 95% of the english sentences have lengths of 57 while in the french, 95 % of sentences have lengths of 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding the sentences with pad_sequences from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 09:42:05.210172: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-15 09:42:05.233194: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-15 09:42:05.252164: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-15 09:42:05.256169: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-15 09:42:05.275226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 09:42:06.148478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>' 'The' 'situation' 'should' 'also' 'remind' 'us' 'all' 'that' 'the'\n",
      " 'ratification' 'of' 'the' 'Treaty' 'for' 'an' 'International' 'Criminal'\n",
      " 'Court' 'has' 'been' 'shamefully' 'slow.' '</s>' 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "n_en_seq_length = 60\n",
    "n_fr_seq_length = 60\n",
    "unk_token = \"<unk>\"\n",
    "\n",
    "train_en_sentences_padded = pad_sequences(\n",
    "    train_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_en_sentences_padded = pad_sequences(\n",
    "    valid_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_en_sentences_padded = pad_sequences(\n",
    "    test_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "\n",
    "train_fr_sentences_padded = pad_sequences(\n",
    "    train_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_fr_sentences_padded = pad_sequences(\n",
    "    valid_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_fr_sentences_padded = pad_sequences(\n",
    "    test_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "print(train_en_sentences_padded[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728999727.254418   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.290720   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.290765   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.294375   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.294421   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.294434   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.527439   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.527537   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-15 09:42:07.527546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1728999727.527578   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-15 09:42:07.527593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import os\n",
    "\n",
    "# using text vectorization\n",
    "# text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_fr = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_en.adapt(data[\"en\"])\n",
    "# text_vectorizer_fr.adapt(data[\"fr\"])\n",
    "\n",
    "en_vocabulary = []\n",
    "with open(os.path.join(\"./data/en-fr\", \"vocab.en\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        en_vocabulary.append(row.strip())\n",
    "\n",
    "fr_vocabulary = []\n",
    "with open(os.path.join(\"./data/en-fr\", \"vocab.fr\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        fr_vocabulary.append(row.strip())\n",
    "\n",
    "text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_fr = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_en.adapt(en_vocabulary)\n",
    "text_vectorizer_fr.adapt(fr_vocabulary)\n",
    "\n",
    "\n",
    "en_vocabulary = text_vectorizer_en.get_vocabulary()\n",
    "fr_vocabulary = text_vectorizer_fr.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[UNK]', '[UNK]')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_unk_token = en_vocabulary.pop(1)\n",
    "fr_unk_token = fr_vocabulary.pop(1)\n",
    "\n",
    "en_unk_token, fr_unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "pad_token = \"[PAD]\"\n",
    "\n",
    "# English look up layer\n",
    "en_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=en_vocabulary,\n",
    "    oov_token=en_unk_token,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")\n",
    "\n",
    "# French look up layer\n",
    "fr_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=fr_vocabulary,\n",
    "    oov_token=en_unk_token,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(en_lookup_layer)\n",
    "# en_lookup_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes n_en_seq_length of sentences\n",
    "encoder_input = tf.keras.layers.Input(shape=(n_en_seq_length,), dtype=tf.string)\n",
    "\n",
    "# using lookup layer into word IDs\n",
    "encoder_wid_out = en_lookup_layer(encoder_input)\n",
    "\n",
    "\"\"\"\n",
    "With the tokens converted into IDs, route the generated word IDs to a token embedding layer.\n",
    "Pass in the size of the vocabulary (derived from the en_lookup_layer's get_vocabulary()\n",
    "method) and the embedding size (128) and finally then ask the layer to mask any zero-valued inputs\n",
    "as they don’t contain any information:\n",
    "\n",
    "\"\"\"\n",
    "en_full_vocab_size = len(en_lookup_layer.get_vocabulary())\n",
    "encoder_emb_out = tf.keras.layers.Embedding(en_full_vocab_size, 128, mask_zero=True)(\n",
    "    encoder_wid_out\n",
    ")\n",
    "\n",
    "\n",
    "encoder_gru_out, encoder_gru_last_state = tf.keras.layers.GRU(\n",
    "    256, return_sequences=True, return_state=True\n",
    ")(encoder_emb_out)\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Decoder with teacher forcing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = tf.keras.layers.Input(shape=(n_fr_seq_length - 1,), dtype=tf.string)\n",
    "\n",
    "# convert tokens to IDs using the de_lookup_layer\n",
    "decoder_wid_out = fr_lookup_layer(decoder_input)\n",
    "\n",
    "# decoder embedding layer\n",
    "fr_full_vocab_size = len(fr_lookup_layer.get_vocabulary())\n",
    "decoder_emb_out = tf.keras.layers.Embedding(fr_full_vocab_size, 128, mask_zero=True)(\n",
    "    decoder_wid_out\n",
    ")\n",
    "\n",
    "# decoder layer>>> pass the last state of the encoder into the decoder\n",
    "decoder_gru_out = tf.keras.layers.GRU(256, return_sequences=True)(\n",
    "    decoder_emb_out, initial_state=encoder_gru_last_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Badanau Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # Weights to compute Bahdanau attention\n",
    "        self.Wa = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.Ua = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention(\n",
    "            use_scale=True\n",
    "        )  # takes query, key and value\n",
    "        # query = each decoder GRU's hidden states for eact time step.\n",
    "\n",
    "    def call(self, query, key, value, mask, return_attention_scores=False):\n",
    "\n",
    "        # compute 'Wa.ht'.\n",
    "\n",
    "        wa_query = self.Wa(query)\n",
    "\n",
    "        # Compute Ua.hs\n",
    "        ua_key = self.Ua(key)\n",
    "\n",
    "        # compute masks\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        # Compute the attention\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs=[wa_query, value, ua_key],\n",
    "            mask=[query_mask, value_mask, value_mask],\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "\n",
    "        if not return_attention_scores:\n",
    "            return context_vector\n",
    "        else:\n",
    "            return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'bahdanau_attention' (of type BahdanauAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "decoder_attn_out, attn_weights = BahdanauAttention(256)(\n",
    "    query=decoder_gru_out,\n",
    "    key=encoder_gru_out,\n",
    "    value=encoder_gru_out,\n",
    "    mask=(encoder_wid_out != 0),  # mask that denotes which tokens need to be ignored\n",
    "    return_attention_scores=True,\n",
    ")\n",
    "\n",
    "\n",
    "# combine the attention output and the decoder's GRU output to create a\n",
    "# single concatenated input for the prediction\n",
    "context_and_gru_output = tf.keras.layers.Concatenate(axis=-1)(\n",
    "    [decoder_attn_out, decoder_gru_out]\n",
    ")\n",
    "\n",
    "# Prediction layer takes the concatenated attention's context vewctore andthe GRU ouput to\n",
    "# produce probability distributions over the French tokens for each time step\n",
    "decoder_out = tf.keras.layers.Dense(fr_full_vocab_size, activation=\"softmax\")(\n",
    "    context_and_gru_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final end-to-end model\n",
    "seq2seq_model = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=decoder_out\n",
    ")\n",
    "\n",
    "seq2seq_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_visualizer = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=[attn_weights, decoder_out]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(fr_lookup_layer, train_xy, valid_xy, test_xy):\n",
    "    \"\"\"\n",
    "    The prepare_data() function takes the source sentence and target sentence pairs and generates\n",
    "    encoder and decoder inputs and decoder labels.\n",
    "\n",
    "    • fr_lookup_layer   =>  The StringLookup layer of the French language\n",
    "    • train_xy          =>  A tuple containing tokenized English sentences and tokenized\n",
    "                            French sentences in the training set, respectively\n",
    "    • valid_xy          =>  Similar to train_xy but for validation data\n",
    "    • test_xy           =>  Similar to train_xy but for test data\n",
    "\n",
    "\n",
    "    For each training, validation, and test dataset, this function generates:\n",
    "\n",
    "    • encoder_inputs => Tokenized English sentences as in the preprocessed dataset\n",
    "    • decoder_inputs => All tokens except the last of each French sentence\n",
    "    • decoder_labels => All token IDs except the first of each French sentence, where token\n",
    "                        IDs are generated by the fr_lookup_layer\n",
    "\n",
    "    decoder_labels will be decoder_inputs shifted one token to the left\n",
    "    \"\"\"\n",
    "    # Create a data dictionary from the dataframes containing data\n",
    "    data_dict = {}\n",
    "    for label, data_xy in zip(\n",
    "        [\"train\", \"valid\", \"test\"], [train_xy, valid_xy, test_xy]\n",
    "    ):\n",
    "        data_x, data_y = data_xy\n",
    "        en_inputs = data_x\n",
    "        fr_inputs = data_y[:, :-1]\n",
    "        fr_labels = fr_lookup_layer(data_y[:, 1:]).numpy()\n",
    "        data_dict[label] = {\n",
    "            \"encoder_input\": en_inputs,\n",
    "            \"decoder_inputs\": fr_inputs,\n",
    "            \"decoder_labels\": fr_labels,\n",
    "        }\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(en_inputs, fr_inputs, fr_labels, shuffle_inds=None):\n",
    "    \"\"\"\n",
    "    Shuffle the data randomly (but all inputs and labels at once)\n",
    "    \"\"\"\n",
    "\n",
    "    if shuffle_inds is None:\n",
    "        # If shuffle_inds are not passed, create a shuffle automatically\n",
    "        shuffle_inds = np.random.permutation(np.arange(en_inputs.shape[0]))\n",
    "    else:\n",
    "        # Shuffle the provided shuffle_inds\n",
    "        shuffle_inds = np.random.permutation(shuffle_inds)\n",
    "\n",
    "    # return shuffled data\n",
    "    return (\n",
    "        en_inputs[shuffle_inds],\n",
    "        fr_inputs[shuffle_inds],\n",
    "        fr_labels[shuffle_inds],\n",
    "    ), shuffle_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:32: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:32: SyntaxWarning: invalid escape sequence '\\/'\n",
      "/tmp/ipykernel_22491/3402280070.py:32: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  \"<\\/s>.*\",\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "from bleu import compute_bleu\n",
    "\n",
    "\n",
    "class BLEUMetric(object):\n",
    "\n",
    "    def __init__(self, vocabulary, name=\"perplexity\", **kwargs):\n",
    "        \"\"\"Computes the BLEU score (Metric for machine translation)\"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab = vocabulary\n",
    "        self.id_to_token_layer = StringLookup(vocabulary=self.vocab, invert=True)\n",
    "\n",
    "    def calculate_bleu_from_predictions(self, real, pred):\n",
    "        \"\"\"Calculate the BLEU score for targets and predictions\"\"\"\n",
    "\n",
    "        # Get the predicted token IDs\n",
    "        pred_argmax = tf.argmax(pred, axis=-1)\n",
    "\n",
    "        # Convert token IDs to words using the vocabulary and the StringLookup\n",
    "        pred_tokens = self.id_to_token_layer(pred_argmax)\n",
    "        real_tokens = self.id_to_token_layer(real)\n",
    "\n",
    "        def clean_text(tokens):\n",
    "            \"\"\"Clean padding and <s>/</s> tokens to only keep meaningful words\"\"\"\n",
    "\n",
    "            # 3. Strip the string of any extra white spaces\n",
    "            translations_in_bytes = tf.strings.strip(\n",
    "                # 2. Replace everything after the eos token with blank\n",
    "                tf.strings.regex_replace(\n",
    "                    # 1. Join all the tokens to one string in each sequence\n",
    "                    tf.strings.join(tf.transpose(tokens), separator=\" \"),\n",
    "                    \"<\\/s>.*\",\n",
    "                    \"\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Decode the byte stream to a string\n",
    "            translations = np.char.decode(\n",
    "                translations_in_bytes.numpy().astype(np.bytes_), encoding=\"utf-8\"\n",
    "            )\n",
    "\n",
    "            # If the string is empty, add a [UNK] token\n",
    "            # Otherwise get a Division by zero error\n",
    "            translations = [\n",
    "                sent if len(sent) > 0 else en_unk_token for sent in translations\n",
    "            ]\n",
    "\n",
    "            # Split the sequences to individual tokens\n",
    "            translations = np.char.split(translations).tolist()\n",
    "\n",
    "            return translations\n",
    "\n",
    "        # Get the clean versions of the predictions and real seuqences\n",
    "        pred_tokens = clean_text(pred_tokens)\n",
    "        # We have to wrap each real sequence in a list to make use of a function to compute bleu\n",
    "        real_tokens = [[token_seq] for token_seq in clean_text(real_tokens)]\n",
    "\n",
    "        # The compute_bleu method accpets the translations and references in the following format\n",
    "        # tranlation - list of list of tokens\n",
    "        # references - list of list of list of tokens\n",
    "        bleu, precisions, bp, ratio, translation_length, reference_length = (\n",
    "            compute_bleu(real_tokens, pred_tokens, smooth=False)\n",
    "        )\n",
    "\n",
    "        return bleu\n",
    "\n",
    "\n",
    "def check_for_nans(loss, model, en_lookup_layer, de_lookup_layer):\n",
    "\n",
    "    if np.isnan(loss):\n",
    "        for r_i in range(len(y)):\n",
    "            loss_sample, _ = model.evaluate(\n",
    "                [x[0][r_i : r_i + 1], x[1][r_i : r_i + 1]], y[r_i : r_i + 1], verbose=0\n",
    "            )\n",
    "            if np.isnan(loss_sample):\n",
    "\n",
    "                print(\"=\" * 25, \"nan detected\", \"=\" * 25)\n",
    "                print(\"train_batch\", i, \"r_i\", r_i)\n",
    "                print(\"en_input ->\", x[0][r_i].tolist())\n",
    "                print(\"en_input_wid ->\", en_lookup_layer(x[0][r_i]).numpy().tolist())\n",
    "                print(\"de_input ->\", x[1][r_i].tolist())\n",
    "                print(\"de_input_wid ->\", de_lookup_layer(x[1][r_i]).numpy().tolist())\n",
    "                print(\"de_output_wid ->\", y[r_i].tolist())\n",
    "\n",
    "                if r_i > 0:\n",
    "                    print(\"=\" * 25, \"no-nan\", \"=\" * 25)\n",
    "                    print(\"en_input ->\", x[0][r_i - 1].tolist())\n",
    "                    print(\n",
    "                        \"en_input_wid ->\",\n",
    "                        en_lookup_layer(x[0][r_i - 1]).numpy().tolist(),\n",
    "                    )\n",
    "                    print(\"de_input ->\", x[1][r_i - 1].tolist())\n",
    "                    print(\n",
    "                        \"de_input_wid ->\",\n",
    "                        de_lookup_layer(x[1][r_i - 1]).numpy().tolist(),\n",
    "                    )\n",
    "                    print(\"de_output_wid ->\", y[r_i - 1].tolist())\n",
    "                    return\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model, de_lookup_layer, en_inputs_raw, de_inputs_raw, de_labels, batch_size\n",
    "):\n",
    "    \"\"\"Evaluate the model on various metrics such as loss, accuracy and BLEU\"\"\"\n",
    "\n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(de_vocabulary)\n",
    "\n",
    "    loss_log, accuracy_log, bleu_log = [], [], []\n",
    "    # Get the number of batches\n",
    "    n_batches = en_inputs_raw.shape[0] // batch_size\n",
    "    print(\" \", end=\"\\r\")\n",
    "\n",
    "    # Evaluate one batch at a time\n",
    "    for i in range(n_batches):\n",
    "        # Status update\n",
    "        print(f\"Evaluating batch {i+1}/{n_batches}\", end=\"\\r\")\n",
    "\n",
    "        # Get the inputs and targers\n",
    "        x = [\n",
    "            en_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "            de_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "        ]\n",
    "        y = de_labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "        # Get the evaluation metrics\n",
    "        loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "        # Get the predictions to compute BLEU\n",
    "        pred_y = model.predict(x)\n",
    "\n",
    "        # Update logs\n",
    "        loss_log.append(loss)\n",
    "        accuracy_log.append(accuracy)\n",
    "        bleu_log.append(bleu_metric.calculate_bleu_from_predictions(y, pred_y))\n",
    "\n",
    "    return np.mean(loss_log), np.mean(accuracy_log), np.mean(bleu_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu import compute_bleu as BLEUMetric\n",
    "import keras_nlp\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    en_lookup_layer,\n",
    "    fr_lookup_layer,\n",
    "    train_xy,\n",
    "    valid_xy,\n",
    "    test_xy,\n",
    "    epochs,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    predict_bleu_at_training=False,\n",
    "):\n",
    "    \"\"\"Training the model and evaluating on validation/test sets\"\"\"\n",
    "\n",
    "    # Define the metric\n",
    "    bleu_metric = BLEUMetric(en_vocabulary)\n",
    "\n",
    "    # Define the data\n",
    "    data_dict = prepare_data(fr_lookup_layer, train_xy, valid_xy, test_xy)\n",
    "\n",
    "    shuffle_inds = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Reset metric logs every epoch\n",
    "        if predict_bleu_at_training:\n",
    "            bleu_log = []\n",
    "        accuracy_log = []\n",
    "        loss_log = []\n",
    "\n",
    "        # ========================================================== #\n",
    "        #                   Train Phase                              #\n",
    "        # ========================================================== #\n",
    "\n",
    "        # shuffle data at the beginning of every epoch\n",
    "        if shuffle:\n",
    "            (en_inputs_raw, fr_inputs_raw, fr_labels), shuffle_inds = shuffle_data(\n",
    "                data_dict[\"train\"][\"encoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_labels\"],\n",
    "                shuffle_inds,\n",
    "            )\n",
    "        else:\n",
    "            (en_inputs_raw, de_inputs_raw, de_labels) = (\n",
    "                data_dict[\"train\"][\"encoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_inputs\"],\n",
    "                data_dict[\"train\"][\"decoder_labels\"],\n",
    "            )\n",
    "\n",
    "        # Get the number of training batches\n",
    "        n_train_batches = en_inputs_raw.shape[0] // batch_size\n",
    "\n",
    "        prev_loss = None\n",
    "        # Train one batch at a time\n",
    "        for i in range(n_train_batches):\n",
    "            # Status update\n",
    "            print(\"Training batch {}/{}\".format(i + 1, n_train_batches), end=\"\\r\")\n",
    "\n",
    "            # Get a batch of inputs (english and french sequences)\n",
    "            x = [\n",
    "                en_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "                fr_inputs_raw[i * batch_size : (i + 1) * batch_size],\n",
    "            ]\n",
    "\n",
    "            # Get a batch of targets (french sequences offset by 1)\n",
    "            y = fr_labels[i * batch_size : (i + 1) * batch_size]\n",
    "\n",
    "            loss, accuracy = model.evaluate(x, y, verbose=0)\n",
    "\n",
    "            # check if any samples are causing NaNs\n",
    "            check_for_nans(loss, model, en_lookup_layer, fr_lookup_layer)\n",
    "\n",
    "            # Train for a single step\n",
    "            model.train_on_batch(x, y)\n",
    "\n",
    "            # Update the epoch's log records of the metrics\n",
    "            loss_log.append(loss)\n",
    "            accuracy_log.append(accuracy)\n",
    "\n",
    "            if predict_bleu_at_training:\n",
    "                # Get the final prediction to compute BLEU\n",
    "                pred_y = model.predict(x)\n",
    "                bleu_log.append(bleu_metric.calculate_bleu_from_predictions(pred_y))\n",
    "\n",
    "            print(\"\")\n",
    "            print(\"\\nEpoch {}/{}\".format(epoch + 1, epochs))\n",
    "            if predict_bleu_at_training:\n",
    "\n",
    "                print(\n",
    "                    f\"\\t(train) loss: {np.mean(loss_log)} - accuracy:{np.mean(accuracy_log)} - bleu: {np.mean(bleu_log)}\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"\\t(train) loss: {np.mean(loss_log)} - accuracy: {np.mean(accuracy_log)}\"\n",
    "                )\n",
    "\n",
    "            # ========================================================== #\n",
    "            #                   Validation Phase                         #\n",
    "            # ========================================================== #\n",
    "\n",
    "            val_en_inputs = data_dict[\"valid\"][\"encoder_inputs\"]\n",
    "            val_fr_inputs = data_dict[\"valid\"][\"decoder_inputs\"]\n",
    "            val_fr_labels = data_dict[\"valid\"][\"decoder_labels\"]\n",
    "\n",
    "            val_loss, val_accuracy, val_bleu = evaluate_model(\n",
    "                model,\n",
    "                fr_lookup_layer,\n",
    "                val_en_inputs,\n",
    "                val_fr_inputs,\n",
    "                val_fr_labels,\n",
    "                batch_size,\n",
    "            )\n",
    "            # Print the evaluation metrics of each epoch\n",
    "            print(\n",
    "                \"\\t(valid) loss: {} - accuracy: {} - bleu: {}\".format(\n",
    "                    val_loss, val_accuracy, val_bleu\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ========================================================== #\n",
    "        #                   test Phase                               #\n",
    "        # ========================================================== #\n",
    "\n",
    "        test_en_inputs = data_dict[\"test\"][\"encoder_inputs\"]\n",
    "        test_fr_inputs = data_dict[\"test\"][\"decoder_inputs\"]\n",
    "        test_fr_labels = data_dict[\"test\"][\"decoder_labels\"]\n",
    "        test_loss, test_accuracy, test_bleu = evaluate_model(\n",
    "            model,\n",
    "            fr_lookup_layer,\n",
    "            test_en_inputs,\n",
    "            test_fr_inputs,\n",
    "            test_fr_labels,\n",
    "            batch_size,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"\\n(test) loss: {} - accuracy: {} - bleu: {}\".format(\n",
    "                test_loss, test_accuracy, test_bleu\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras_nlp.src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_nlp_export\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_float_dtype\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_to_list\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_nlp.src'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
