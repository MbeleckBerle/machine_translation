{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# The data\n",
    "\n",
    "https://www.kaggle.com/datasets/mohamedlotfy50/wmt-2014-english-french/data\n",
    "There over 4.5 million sentence pairs available. However, I will only use 25,000 pairs due to computational feasiblility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Reprise de la session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Je déclare reprise la session du Parlement eur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Comme vous avez pu le constater, le grand \"bog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Vous avez souhaité un débat à ce sujet dans le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>En attendant, je souhaiterais, comme un certai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                                  fr  \n",
       "0                              Reprise de la session  \n",
       "1  Je déclare reprise la session du Parlement eur...  \n",
       "2  Comme vous avez pu le constater, le grand \"bog...  \n",
       "3  Vous avez souhaité un débat à ce sujet dans le...  \n",
       "4  En attendant, je souhaiterais, comme un certai...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "n_sentences = 25000\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"./data/en-fr/wmt14_translate_fr-en_train.csv\", nrows=n_sentences\n",
    ").dropna()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spliting the sentences into tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  I completely understand your concerns, and I feel that there is a need for effort and a certain degree of consistency within the European Union itself on this issue.\n",
      "French:  Je comprends parfaitement les inquiétudes de M. le député et je vois bien que l'on a besoin d'un effort et d'une certaine cohérence y compris à l'intérieur de l'Union européenne en ce qui concerne cette matière. \n",
      "\n",
      "English:  If the world' s other important currency trading areas were to be excluded from the Tobin tax, the fact that tax was liable to be paid in Europe would lead to currency dealing moving to those areas.\n",
      "French:  Si les autres zones de change importantes du monde restaient en dehors de la taxe Tobin, la taxe perçue en Europe aurait pour effet de transférer le marché des changes vers ces zones-là. \n",
      "\n",
      "English:  Three amendments can be partially approved, and one can be approved in principle.\n",
      "French:  Trois amendements peuvent être partiellement adoptés et un peut l'être en principe. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "original_en_sentences = [sent.strip().split(\" \") for sent in data[\"en\"]]\n",
    "original_fr_sentences = [sent.strip().split(\" \") for sent in data[\"fr\"]]\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(original_en_sentences[index]))\n",
    "    print(\"French: \", \" \".join(original_fr_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding special tokens\n",
    "\n",
    "#### I will add \"< s >\" to mark the start of a sentence and \"< /s >\" to mark the end of a sentence\n",
    "\n",
    "This way\n",
    "we prediction can be done for an arbitrary number of time steps. Using < s > as the starting token gives a\n",
    "way to signal to the decoder that it should start predicting tokens from the target language.\n",
    "\n",
    "if < /s > token is not used to mark the end of a sentence, the decoder cannot be signaled to\n",
    "end a sentence. This can lead the model to enter an infinite loop of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  <s> Not only do these decisions conflict with the Treaty and apportion to the institutions of the Union more power than that to which they are entitled but, worst of all, their effect will be counter-productive. </s>\n",
      "German:  <s> Les adoptions de ce jour sont non seulement contraires au Traité et octroient aux organes de l'Union davantage de pouvoirs que ceux qui lui reviennent, mais surtout, elles produiront un effet contraire. </s> \n",
      "\n",
      "English:  <s> I also believe that the Commission, or President Prodi, has acted as a guardian of the Treaties. </s>\n",
      "German:  <s> En outre, je pense que la Commission, Monsieur le Président Prodi, a été gardienne des Traités. </s> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_en_sentences]\n",
    "fr_sentences = [[\"<s>\"] + sent + [\"</s>\"] for sent in original_fr_sentences]\n",
    "\n",
    "for i in range(2):\n",
    "    index = random.randint(0, 10000)\n",
    "    print(\"English: \", \" \".join(en_sentences[index]))\n",
    "    print(\"German: \", \" \".join(fr_sentences[index]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting training and validation dataset\n",
    "\n",
    "#### 80% training, 10% validation and 10% for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'The', 'situation', 'should', 'also', 'remind', 'us', 'all', 'that', 'the', 'ratification', 'of', 'the', 'Treaty', 'for', 'an', 'International', 'Criminal', 'Court', 'has', 'been', 'shamefully', 'slow.', '</s>']\n",
      "['<s>', 'La', 'situation', 'devrait', 'nous', 'rappeler', 'aussi', 'le', 'fait', 'que', 'la', 'ratification', 'du', 'traité', 'instituant', 'le', 'Tribunal', 'pénal', 'international', 'a', 'été', 'honteusement', 'lente.', '</s>']\n",
      "\n",
      "\n",
      "['<s>', 'I', 'must', 'say', 'that', 'Alex', \"Langer's\", 'presence', 'here', 'in', 'the', 'European', 'Parliament', 'is', 'sadly', 'missed.', 'His', 'efforts', 'to', 'build', 'peace', 'should', 'serve', 'as', 'an', 'example,', 'to', 'all', 'of', 'us,', 'on', 'how', 'peaceful', 'coexistence', 'can', 'be', 'achieved.', '</s>']\n",
      "['<s>', 'Je', 'tiens', 'à', 'dire', 'que', 'je', 'ressens', 'fortement', \"l'absence\", 'en', 'ce', 'moment,', 'dans', 'ce', 'Parlement,', \"d'Alex\", 'Langer,', 'qui', 'a', 'été', 'un', 'architecte', 'de', 'la', 'paix', 'et', 'qui', 'nous', 'a', 'montré', 'à', 'tous', 'comment', 'on', 'instaure', 'la', 'cohabitation', 'entre', 'les', 'peuples.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "(\n",
    "    train_en_sentences,\n",
    "    valid_test_en_sentences,\n",
    "    train_fr_sentences,\n",
    "    valid_test_fr_sentences,\n",
    ") = train_test_split(en_sentences, fr_sentences, test_size=0.2)\n",
    "\n",
    "\n",
    "(valid_en_sentences, test_en_sentences, valid_fr_sentences, test_fr_sentences) = (\n",
    "    train_test_split(valid_test_en_sentences, valid_test_fr_sentences, test_size=0.5)\n",
    ")\n",
    "\n",
    "\n",
    "print(train_en_sentences[1])\n",
    "print(train_fr_sentences[1])\n",
    "print(\"\\n\")\n",
    "print(test_en_sentences[0])\n",
    "print(test_fr_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining sequence leghts fot the two languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        27.699450\n",
       "std         15.769621\n",
       "min          3.000000\n",
       "5%           8.000000\n",
       "50%         25.000000\n",
       "95%         58.000000\n",
       "max        150.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting some basic statistics from the data\n",
    "\n",
    "# convert train_en_sentences to a pandas series\n",
    "pd.Series(train_en_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean        28.995000\n",
       "std         16.805437\n",
       "min          3.000000\n",
       "5%           9.000000\n",
       "50%         26.000000\n",
       "95%         60.000000\n",
       "max        154.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_fr_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from the train data statistics above, 95% of the english sentences have lengths of 57 while in the french, 95 % of sentences have lengths of 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding the sentences with pad_sequences from keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 09:42:05.210172: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-15 09:42:05.233194: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-15 09:42:05.252164: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-15 09:42:05.256169: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-15 09:42:05.275226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 09:42:06.148478: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>' 'The' 'situation' 'should' 'also' 'remind' 'us' 'all' 'that' 'the'\n",
      " 'ratification' 'of' 'the' 'Treaty' 'for' 'an' 'International' 'Criminal'\n",
      " 'Court' 'has' 'been' 'shamefully' 'slow.' '</s>' 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0\n",
      " 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "n_en_seq_length = 60\n",
    "n_fr_seq_length = 60\n",
    "unk_token = \"<unk>\"\n",
    "\n",
    "train_en_sentences_padded = pad_sequences(\n",
    "    train_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_en_sentences_padded = pad_sequences(\n",
    "    valid_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_en_sentences_padded = pad_sequences(\n",
    "    test_en_sentences,\n",
    "    maxlen=n_en_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "\n",
    "train_fr_sentences_padded = pad_sequences(\n",
    "    train_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "valid_fr_sentences_padded = pad_sequences(\n",
    "    valid_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "test_fr_sentences_padded = pad_sequences(\n",
    "    test_fr_sentences,\n",
    "    maxlen=n_fr_seq_length,\n",
    "    # value=unk_token,\n",
    "    dtype=object,\n",
    "    truncating=\"post\",\n",
    "    padding=\"post\",\n",
    ")\n",
    "\n",
    "print(train_en_sentences_padded[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728999727.254418   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.290720   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.290765   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.294375   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.294421   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.294434   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.527439   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728999727.527537   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-15 09:42:07.527546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1728999727.527578   22491 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-15 09:42:07.527593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5520 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import os\n",
    "\n",
    "# using text vectorization\n",
    "# text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_fr = TextVectorization(output_mode=\"int\")\n",
    "# text_vectorizer_en.adapt(data[\"en\"])\n",
    "# text_vectorizer_fr.adapt(data[\"fr\"])\n",
    "\n",
    "en_vocabulary = []\n",
    "with open(os.path.join(\"./data/en-fr\", \"vocab.en\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        en_vocabulary.append(row.strip())\n",
    "\n",
    "fr_vocabulary = []\n",
    "with open(os.path.join(\"./data/en-fr\", \"vocab.fr\"), \"r\", encoding=\"utf-8\") as en_file:\n",
    "    for ri, row in enumerate(en_file):\n",
    "\n",
    "        fr_vocabulary.append(row.strip())\n",
    "\n",
    "text_vectorizer_en = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_fr = TextVectorization(output_mode=\"int\")\n",
    "text_vectorizer_en.adapt(en_vocabulary)\n",
    "text_vectorizer_fr.adapt(fr_vocabulary)\n",
    "\n",
    "\n",
    "en_vocabulary = text_vectorizer_en.get_vocabulary()\n",
    "fr_vocabulary = text_vectorizer_fr.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[UNK]', '[UNK]')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_unk_token = en_vocabulary.pop(1)\n",
    "fr_unk_token = fr_vocabulary.pop(1)\n",
    "\n",
    "en_unk_token, fr_unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "pad_token = \"[PAD]\"\n",
    "\n",
    "# English look up layer\n",
    "en_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=en_vocabulary,\n",
    "    oov_token=en_unk_token,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")\n",
    "\n",
    "# French look up layer\n",
    "fr_lookup_layer = tf.keras.layers.StringLookup(\n",
    "    vocabulary=fr_vocabulary,\n",
    "    oov_token=en_unk_token,\n",
    "    mask_token=pad_token,\n",
    "    pad_to_max_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(en_lookup_layer)\n",
    "# en_lookup_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes n_en_seq_length of sentences\n",
    "encoder_input = tf.keras.layers.Input(shape=(n_en_seq_length,), dtype=tf.string)\n",
    "\n",
    "# using lookup layer into word IDs\n",
    "encoder_wid_out = en_lookup_layer(encoder_input)\n",
    "\n",
    "\"\"\"\n",
    "With the tokens converted into IDs, route the generated word IDs to a token embedding layer.\n",
    "Pass in the size of the vocabulary (derived from the en_lookup_layer's get_vocabulary()\n",
    "method) and the embedding size (128) and finally then ask the layer to mask any zero-valued inputs\n",
    "as they don’t contain any information:\n",
    "\n",
    "\"\"\"\n",
    "en_full_vocab_size = len(en_lookup_layer.get_vocabulary())\n",
    "encoder_emb_out = tf.keras.layers.Embedding(en_full_vocab_size, 128, mask_zero=True)(\n",
    "    encoder_wid_out\n",
    ")\n",
    "\n",
    "\n",
    "encoder_gru_out, encoder_gru_last_state = tf.keras.layers.GRU(\n",
    "    256, return_sequences=True, return_state=True\n",
    ")(encoder_emb_out)\n",
    "\n",
    "encoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Decoder with teacher forcing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = tf.keras.layers.Input(shape=(n_fr_seq_length - 1,), dtype=tf.string)\n",
    "\n",
    "# convert tokens to IDs using the de_lookup_layer\n",
    "decoder_wid_out = fr_lookup_layer(decoder_input)\n",
    "\n",
    "# decoder embedding layer\n",
    "fr_full_vocab_size = len(fr_lookup_layer.get_vocabulary())\n",
    "decoder_emb_out = tf.keras.layers.Embedding(fr_full_vocab_size, 128, mask_zero=True)(\n",
    "    decoder_wid_out\n",
    ")\n",
    "\n",
    "# decoder layer>>> pass the last state of the encoder into the decoder\n",
    "decoder_gru_out = tf.keras.layers.GRU(256, return_sequences=True)(\n",
    "    decoder_emb_out, initial_state=encoder_gru_last_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Badanau Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        # Weights to compute Bahdanau attention\n",
    "        self.Wa = tf.keras.layers.Dense(units, use_bias=False)\n",
    "        self.Ua = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "        self.attention = tf.keras.layers.AdditiveAttention(\n",
    "            use_scale=True\n",
    "        )  # takes query, key and value\n",
    "        # query = each decoder GRU's hidden states for eact time step.\n",
    "\n",
    "    def call(self, query, key, value, mask, return_attention_scores=False):\n",
    "\n",
    "        # compute 'Wa.ht'.\n",
    "\n",
    "        wa_query = self.Wa(query)\n",
    "\n",
    "        # Compute Ua.hs\n",
    "        ua_key = self.Ua(key)\n",
    "\n",
    "        # compute masks\n",
    "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "        value_mask = mask\n",
    "\n",
    "        # Compute the attention\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            inputs=[wa_query, value, ua_key],\n",
    "            mask=[query_mask, value_mask, value_mask],\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "\n",
    "        if not return_attention_scores:\n",
    "            return context_vector\n",
    "        else:\n",
    "            return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbeleck/anaconda3/envs/tf2-cuda/lib/python3.12/site-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'bahdanau_attention' (of type BahdanauAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "decoder_attn_out, attn_weights = BahdanauAttention(256)(\n",
    "    query=decoder_gru_out,\n",
    "    key=encoder_gru_out,\n",
    "    value=encoder_gru_out,\n",
    "    mask=(encoder_wid_out != 0),  # mask that denotes which tokens need to be ignored\n",
    "    return_attention_scores=True,\n",
    ")\n",
    "\n",
    "\n",
    "# combine the attention output and the decoder's GRU output to create a\n",
    "# single concatenated input for the prediction\n",
    "context_and_gru_output = tf.keras.layers.Concatenate(axis=-1)(\n",
    "    [decoder_attn_out, decoder_gru_out]\n",
    ")\n",
    "\n",
    "# Prediction layer takes the concatenated attention's context vewctore andthe GRU ouput to\n",
    "# produce probability distributions over the French tokens for each time step\n",
    "decoder_out = tf.keras.layers.Dense(fr_full_vocab_size, activation=\"softmax\")(\n",
    "    context_and_gru_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final end-to-end model\n",
    "seq2seq_model = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=decoder_out\n",
    ")\n",
    "\n",
    "seq2seq_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_visualizer = tf.keras.models.Model(\n",
    "    inputs=[encoder.inputs, decoder_input], outputs=[attn_weights, decoder_out]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
